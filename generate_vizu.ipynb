{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'checkpoints/checkpoint-2000'\n",
    "#path = 'surrey-nlp/roberta-base-finetuned-abbr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
    "TEXT2ID = {\n",
    "    \"B-O\": 0,\n",
    "    \"B-AC\": 1,\n",
    "    \"B-LF\": 2,\n",
    "    \"I-LF\": 3,\n",
    "}\n",
    "datasets = datasets.map(lambda x: {\"ner_tags\": [TEXT2ID[tag] for tag in x[\"ner_tags\"]]})\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if True else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(path, num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\"ner\", model=model, tokenizer=tokenizer, ignore_labels=[])\n",
    "\n",
    "def choose(i=None):\n",
    "    if i is None:\n",
    "        i = torch.randint(0, len(datasets[\"test\"][\"tokens\"]), (1,)).item()\n",
    "    output = pipeline(\" \".join(datasets[\"test\"][\"tokens\"][i]))\n",
    "    words = datasets[\"test\"][\"tokens\"][i]\n",
    "    truth = datasets[\"test\"][\"ner_tags\"][i]\n",
    "\n",
    "    return words, output, truth\n",
    "\n",
    "def choose_multiple(nb=5):\n",
    "    indices = torch.randint(0, len(datasets[\"test\"][\"tokens\"]), (nb,))\n",
    "    words = []\n",
    "    outputs = []\n",
    "    truths = []\n",
    "    for i in indices:\n",
    "        w, o, t = choose(i)\n",
    "        words.append(w)\n",
    "        outputs.append(o)\n",
    "        truths.append(t)\n",
    "    return words, outputs, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Back, Style\n",
    "\n",
    "TEXT2ID = {\n",
    "    \"O\": 0,\n",
    "    \"B-AC\": 1,\n",
    "    \"B-LF\": 2,\n",
    "    \"I-LF\": 3,\n",
    "}\n",
    "\n",
    "def vizu(words, output, truth, type=None):\n",
    "    sentence = \" \".join(words)\n",
    "    out_words = []\n",
    "    out_label = []\n",
    "    out_truth = []\n",
    "    index = 1\n",
    "    for i in range(len(output)):\n",
    "        start = output[i]['start']\n",
    "        end = output[i]['end']\n",
    "        word = output[i]['word']\n",
    "        if type==1 and 'Ġ' in word: \n",
    "            out_words.append(' ')\n",
    "            out_label.append(0)\n",
    "            index += 1\n",
    "        elif type!=1 and word[0] != '#':\n",
    "            out_words.append(' ')\n",
    "            out_label.append(0)\n",
    "            index += 1\n",
    "        out_words.append(sentence[start:end])\n",
    "        if type==1:\n",
    "            #print(output[i]['entity'])\n",
    "            out_label.append(TEXT2ID[output[i]['entity']])\n",
    "        else:\n",
    "            out_label.append(int(output[i]['entity'][-1]))\n",
    "    col = {0: Back.BLACK, 1: Back.RED, 2: Back.GREEN, 3: Back.BLUE, 4: Back.MAGENTA}\n",
    "    out_label = out_label[1:]\n",
    "    out_words = out_words[1:]\n",
    "    print('Output:  ', end='')\n",
    "    for i in range(len(out_words)):\n",
    "        print(col[out_label[i]], end='')\n",
    "        print(out_words[i], end='')\n",
    "        print(Style.RESET_ALL, end='')\n",
    "    print()\n",
    "    print('Truth:   ', end='')\n",
    "    for i in range(len(words)):\n",
    "        print(col[truth[i]], end='')\n",
    "        print(words[i] + ' ', end='')\n",
    "        print(Style.RESET_ALL, end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  \u001b[40mWhile\u001b[0m\u001b[40m \u001b[0m\u001b[41mF\u001b[0m\u001b[41mGF\u001b[0m\u001b[40m \u001b[0m\u001b[40mprimarily\u001b[0m\u001b[40m \u001b[0m\u001b[40minduce\u001b[0m\u001b[40ms\u001b[0m\u001b[40m \u001b[0m\u001b[40mthe\u001b[0m\u001b[40m \u001b[0m\u001b[42mRas\u001b[0m\u001b[40m \u001b[0m\u001b[44m–\u001b[0m\u001b[40m \u001b[0m\u001b[44mMit\u001b[0m\u001b[44mogen\u001b[0m\u001b[40m \u001b[0m\u001b[44m-\u001b[0m\u001b[40m \u001b[0m\u001b[44mActivated\u001b[0m\u001b[40m \u001b[0m\u001b[44mProtein\u001b[0m\u001b[40m \u001b[0m\u001b[44mKinase\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[41mMAP\u001b[0m\u001b[41mK\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40maxis\u001b[0m\u001b[40m \u001b[0m\u001b[40mto\u001b[0m\u001b[40m \u001b[0m\u001b[40mpromote\u001b[0m\u001b[40m \u001b[0m\u001b[40mlens\u001b[0m\u001b[40m \u001b[0m\u001b[40mcell\u001b[0m\u001b[40m \u001b[0m\u001b[40mdifferentiation\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[41mPD\u001b[0m\u001b[41mGF\u001b[0m\u001b[40m \u001b[0m\u001b[40mprefer\u001b[0m\u001b[40mential\u001b[0m\u001b[40mly\u001b[0m\u001b[40m \u001b[0m\u001b[40mstimulate\u001b[0m\u001b[40ms\u001b[0m\u001b[40m \u001b[0m\u001b[42mPh\u001b[0m\u001b[42mos\u001b[0m\u001b[42mph\u001b[0m\u001b[42moin\u001b[0m\u001b[42mosi\u001b[0m\u001b[42mti\u001b[0m\u001b[42mde\u001b[0m\u001b[40m \u001b[0m\u001b[44m3\u001b[0m\u001b[40m \u001b[0m\u001b[44m-\u001b[0m\u001b[40m \u001b[0m\u001b[44mkinase\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[41mPI\u001b[0m\u001b[41m3\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40mto\u001b[0m\u001b[40m \u001b[0m\u001b[40menhance\u001b[0m\u001b[40m \u001b[0m\u001b[40mNotch\u001b[0m\u001b[40m \u001b[0m\u001b[40msignaling\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40mwhich\u001b[0m\u001b[40m \u001b[0m\u001b[40mis\u001b[0m\u001b[40m \u001b[0m\u001b[40mnecessary\u001b[0m\u001b[40m \u001b[0m\u001b[40mfor\u001b[0m\u001b[40m \u001b[0m\u001b[40mmaintaining\u001b[0m\u001b[40m \u001b[0m\u001b[40mthe\u001b[0m\u001b[40m \u001b[0m\u001b[40mlens\u001b[0m\u001b[40m \u001b[0m\u001b[40mpro\u001b[0m\u001b[40mgen\u001b[0m\u001b[40mitor\u001b[0m\u001b[40m \u001b[0m\u001b[40mcell\u001b[0m\u001b[40m \u001b[0m\u001b[40mpool\u001b[0m\u001b[40m \u001b[0m\u001b[40m.\u001b[0m\n",
      "Truth:   \u001b[40mWhile \u001b[0m\u001b[41mFGF \u001b[0m\u001b[40mprimarily \u001b[0m\u001b[40minduces \u001b[0m\u001b[40mthe \u001b[0m\u001b[40mRas \u001b[0m\u001b[40m– \u001b[0m\u001b[42mMitogen \u001b[0m\u001b[44m- \u001b[0m\u001b[44mActivated \u001b[0m\u001b[44mProtein \u001b[0m\u001b[44mKinase \u001b[0m\u001b[40m( \u001b[0m\u001b[41mMAPK \u001b[0m\u001b[40m) \u001b[0m\u001b[40maxis \u001b[0m\u001b[40mto \u001b[0m\u001b[40mpromote \u001b[0m\u001b[40mlens \u001b[0m\u001b[40mcell \u001b[0m\u001b[40mdifferentiation \u001b[0m\u001b[40m, \u001b[0m\u001b[41mPDGF \u001b[0m\u001b[40mpreferentially \u001b[0m\u001b[40mstimulates \u001b[0m\u001b[42mPhosphoinositide \u001b[0m\u001b[44m3 \u001b[0m\u001b[44m- \u001b[0m\u001b[44mkinase \u001b[0m\u001b[40m( \u001b[0m\u001b[41mPI3 \u001b[0m\u001b[40m) \u001b[0m\u001b[40mto \u001b[0m\u001b[40menhance \u001b[0m\u001b[40mNotch \u001b[0m\u001b[40msignaling \u001b[0m\u001b[40m, \u001b[0m\u001b[40mwhich \u001b[0m\u001b[40mis \u001b[0m\u001b[40mnecessary \u001b[0m\u001b[40mfor \u001b[0m\u001b[40mmaintaining \u001b[0m\u001b[40mthe \u001b[0m\u001b[40mlens \u001b[0m\u001b[40mprogenitor \u001b[0m\u001b[40mcell \u001b[0m\u001b[40mpool \u001b[0m\u001b[40m. \u001b[0m\n",
      "\n",
      "Output:  \u001b[40mL\u001b[0m\u001b[40mym\u001b[0m\u001b[40mph\u001b[0m\u001b[40mocytes\u001b[0m\u001b[40m \u001b[0m\u001b[40mwere\u001b[0m\u001b[40m \u001b[0m\u001b[40mclearly\u001b[0m\u001b[40m \u001b[0m\u001b[40mcontrasted\u001b[0m\u001b[40m \u001b[0m\u001b[40min\u001b[0m\u001b[40m \u001b[0m\u001b[40mthe\u001b[0m\u001b[40m \u001b[0m\u001b[40mmid\u001b[0m\u001b[40mgut\u001b[0m\u001b[40m \u001b[0m\u001b[40mof\u001b[0m\u001b[40m \u001b[0m\u001b[40mtrans\u001b[0m\u001b[40mgenic\u001b[0m\u001b[40m \u001b[0m\u001b[40mmosquito\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[40mFigure\u001b[0m\u001b[40m \u001b[0m\u001b[40m5\u001b[0m\u001b[40mD\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40mbut\u001b[0m\u001b[40m \u001b[0m\u001b[40mnot\u001b[0m\u001b[40m \u001b[0m\u001b[40mamongst\u001b[0m\u001b[40m \u001b[0m\u001b[40mthe\u001b[0m\u001b[40m \u001b[0m\u001b[40mintact\u001b[0m\u001b[40m \u001b[0m\u001b[40mer\u001b[0m\u001b[40myt\u001b[0m\u001b[40mhr\u001b[0m\u001b[40mocytes\u001b[0m\u001b[40m \u001b[0m\u001b[40min\u001b[0m\u001b[40m \u001b[0m\u001b[40mthe\u001b[0m\u001b[40m \u001b[0m\u001b[40mmid\u001b[0m\u001b[40mgut\u001b[0m\u001b[40m \u001b[0m\u001b[40mof\u001b[0m\u001b[40m \u001b[0m\u001b[40mnon\u001b[0m\u001b[40m \u001b[0m\u001b[40m-\u001b[0m\u001b[40m \u001b[0m\u001b[40mtrans\u001b[0m\u001b[40mgenic\u001b[0m\u001b[40m \u001b[0m\u001b[40mmosquito\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[40mFigure\u001b[0m\u001b[40m \u001b[0m\u001b[40m5\u001b[0m\u001b[40mC\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40m.\u001b[0m\n",
      "Truth:   \u001b[40mLymphocytes \u001b[0m\u001b[40mwere \u001b[0m\u001b[40mclearly \u001b[0m\u001b[40mcontrasted \u001b[0m\u001b[40min \u001b[0m\u001b[40mthe \u001b[0m\u001b[40mmidgut \u001b[0m\u001b[40mof \u001b[0m\u001b[40mtransgenic \u001b[0m\u001b[40mmosquito \u001b[0m\u001b[40m( \u001b[0m\u001b[40mFigure \u001b[0m\u001b[40m5D \u001b[0m\u001b[40m) \u001b[0m\u001b[40m, \u001b[0m\u001b[40mbut \u001b[0m\u001b[40mnot \u001b[0m\u001b[40mamongst \u001b[0m\u001b[40mthe \u001b[0m\u001b[40mintact \u001b[0m\u001b[40merythrocytes \u001b[0m\u001b[40min \u001b[0m\u001b[40mthe \u001b[0m\u001b[40mmidgut \u001b[0m\u001b[40mof \u001b[0m\u001b[40mnon \u001b[0m\u001b[40m- \u001b[0m\u001b[40mtransgenic \u001b[0m\u001b[40mmosquito \u001b[0m\u001b[40m( \u001b[0m\u001b[40mFigure \u001b[0m\u001b[40m5C \u001b[0m\u001b[40m) \u001b[0m\u001b[40m. \u001b[0m\n",
      "\n",
      "Output:  \u001b[40mIn\u001b[0m\u001b[40m \u001b[0m\u001b[42mlow\u001b[0m\u001b[40m \u001b[0m\u001b[44mresource\u001b[0m\u001b[40m \u001b[0m\u001b[44msettings\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[41mL\u001b[0m\u001b[41mRS\u001b[0m\u001b[41ms\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40mtreatment\u001b[0m\u001b[40m \u001b[0m\u001b[40mof\u001b[0m\u001b[40m \u001b[0m\u001b[40mneon\u001b[0m\u001b[40matal\u001b[0m\u001b[40m \u001b[0m\u001b[40mja\u001b[0m\u001b[40mund\u001b[0m\u001b[40mice\u001b[0m\u001b[40m \u001b[0m\u001b[40mis\u001b[0m\u001b[40m \u001b[0m\u001b[40moften\u001b[0m\u001b[40m \u001b[0m\u001b[40msub\u001b[0m\u001b[40m \u001b[0m\u001b[40m-\u001b[0m\u001b[40m \u001b[0m\u001b[40moptimal\u001b[0m\u001b[40m \u001b[0m\u001b[40m[\u001b[0m\u001b[40m \u001b[0m\u001b[40m4\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40m5\u001b[0m\u001b[40m \u001b[0m\u001b[40m]\u001b[0m\u001b[40m \u001b[0m\u001b[40m.\u001b[0m\n",
      "Truth:   \u001b[40mIn \u001b[0m\u001b[42mlow \u001b[0m\u001b[44mresource \u001b[0m\u001b[44msettings \u001b[0m\u001b[40m( \u001b[0m\u001b[41mLRSs \u001b[0m\u001b[40m) \u001b[0m\u001b[40m, \u001b[0m\u001b[40mtreatment \u001b[0m\u001b[40mof \u001b[0m\u001b[40mneonatal \u001b[0m\u001b[40mjaundice \u001b[0m\u001b[40mis \u001b[0m\u001b[40moften \u001b[0m\u001b[40msub \u001b[0m\u001b[40m- \u001b[0m\u001b[40moptimal \u001b[0m\u001b[40m[ \u001b[0m\u001b[40m4 \u001b[0m\u001b[40m, \u001b[0m\u001b[40m5 \u001b[0m\u001b[40m] \u001b[0m\u001b[40m. \u001b[0m\n",
      "\n",
      "Output:  \u001b[40mInter\u001b[0m\u001b[40m \u001b[0m\u001b[40m-\u001b[0m\u001b[40m \u001b[0m\u001b[40mand\u001b[0m\u001b[40m \u001b[0m\u001b[40mintra\u001b[0m\u001b[40m \u001b[0m\u001b[40m-\u001b[0m\u001b[40m \u001b[0m\u001b[40mrate\u001b[0m\u001b[40mr\u001b[0m\u001b[40m \u001b[0m\u001b[40mreliability\u001b[0m\u001b[40m \u001b[0m\u001b[40mwas\u001b[0m\u001b[40m \u001b[0m\u001b[40mdetermined\u001b[0m\u001b[40m \u001b[0m\u001b[40mby\u001b[0m\u001b[40m \u001b[0m\u001b[40mmeans\u001b[0m\u001b[40m \u001b[0m\u001b[40mof\u001b[0m\u001b[40m \u001b[0m\u001b[42mintra\u001b[0m\u001b[42mclass\u001b[0m\u001b[40m \u001b[0m\u001b[44mcorrelation\u001b[0m\u001b[40m \u001b[0m\u001b[44mcoefficients\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[41mICC\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40musing\u001b[0m\u001b[40m \u001b[0m\u001b[40mR\u001b[0m\u001b[40m \u001b[0m\u001b[40mStudio\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[40mVersion\u001b[0m\u001b[40m \u001b[0m\u001b[40m1\u001b[0m\u001b[40m \u001b[0m\u001b[40m.\u001b[0m\u001b[40m \u001b[0m\u001b[40m1\u001b[0m\u001b[40m \u001b[0m\u001b[40m.\u001b[0m\u001b[40m \u001b[0m\u001b[40m45\u001b[0m\u001b[40m6\u001b[0m\u001b[40m \u001b[0m\u001b[40m;\u001b[0m\u001b[40m \u001b[0m\u001b[40mRS\u001b[0m\u001b[40mtu\u001b[0m\u001b[40mdio\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40mbased\u001b[0m\u001b[40m \u001b[0m\u001b[40mon\u001b[0m\u001b[40m \u001b[0m\u001b[40ma\u001b[0m\u001b[40m \u001b[0m\u001b[40mmean\u001b[0m\u001b[40m \u001b[0m\u001b[40mof\u001b[0m\u001b[40m \u001b[0m\u001b[40m2\u001b[0m\u001b[40m \u001b[0m\u001b[40mrate\u001b[0m\u001b[40mrs\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40mconsistency\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40m2\u001b[0m\u001b[40m \u001b[0m\u001b[40m-\u001b[0m\u001b[40m \u001b[0m\u001b[40mway\u001b[0m\u001b[40m \u001b[0m\u001b[40mmixed\u001b[0m\u001b[40m \u001b[0m\u001b[40m-\u001b[0m\u001b[40m \u001b[0m\u001b[40meffects\u001b[0m\u001b[40m \u001b[0m\u001b[40mmodel\u001b[0m\u001b[40m \u001b[0m\u001b[40mfor\u001b[0m\u001b[40m \u001b[0m\u001b[40mthe\u001b[0m\u001b[40m \u001b[0m\u001b[40mqu\u001b[0m\u001b[40mali\u001b[0m\u001b[40mtative\u001b[0m\u001b[40m \u001b[0m\u001b[40manalysis\u001b[0m\u001b[40m \u001b[0m\u001b[40m[\u001b[0m\u001b[40m \u001b[0m\u001b[40m42\u001b[0m\u001b[40m \u001b[0m\u001b[40m]\u001b[0m\u001b[40m \u001b[0m\u001b[40m.\u001b[0m\n",
      "Truth:   \u001b[40mInter- \u001b[0m\u001b[40mand \u001b[0m\u001b[40mintra \u001b[0m\u001b[40m- \u001b[0m\u001b[40mrater \u001b[0m\u001b[40mreliability \u001b[0m\u001b[40mwas \u001b[0m\u001b[40mdetermined \u001b[0m\u001b[40mby \u001b[0m\u001b[40mmeans \u001b[0m\u001b[40mof \u001b[0m\u001b[42mintraclass \u001b[0m\u001b[44mcorrelation \u001b[0m\u001b[44mcoefficients \u001b[0m\u001b[40m( \u001b[0m\u001b[41mICC \u001b[0m\u001b[40m) \u001b[0m\u001b[40musing \u001b[0m\u001b[40mR \u001b[0m\u001b[40mStudio \u001b[0m\u001b[40m( \u001b[0m\u001b[40mVersion \u001b[0m\u001b[40m1.1.456 \u001b[0m\u001b[40m; \u001b[0m\u001b[40mRStudio \u001b[0m\u001b[40m) \u001b[0m\u001b[40mbased \u001b[0m\u001b[40mon \u001b[0m\u001b[40ma \u001b[0m\u001b[40mmean \u001b[0m\u001b[40mof \u001b[0m\u001b[40m2 \u001b[0m\u001b[40mraters \u001b[0m\u001b[40m, \u001b[0m\u001b[40mconsistency \u001b[0m\u001b[40m, \u001b[0m\u001b[40m2 \u001b[0m\u001b[40m- \u001b[0m\u001b[40mway \u001b[0m\u001b[40mmixed \u001b[0m\u001b[40m- \u001b[0m\u001b[40meffects \u001b[0m\u001b[40mmodel \u001b[0m\u001b[40mfor \u001b[0m\u001b[40mthe \u001b[0m\u001b[40mqualitative \u001b[0m\u001b[40manalysis \u001b[0m\u001b[40m[ \u001b[0m\u001b[40m42 \u001b[0m\u001b[40m] \u001b[0m\u001b[40m. \u001b[0m\n",
      "\n",
      "Output:  \u001b[40mMosquito\u001b[0m\u001b[40mes\u001b[0m\u001b[40m \u001b[0m\u001b[40mwere\u001b[0m\u001b[40m \u001b[0m\u001b[40mallowed\u001b[0m\u001b[40m \u001b[0m\u001b[40mto\u001b[0m\u001b[40m \u001b[0m\u001b[40mfeed\u001b[0m\u001b[40m \u001b[0m\u001b[40mon\u001b[0m\u001b[40m \u001b[0m\u001b[40ma\u001b[0m\u001b[40m \u001b[0m\u001b[40mhuman\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40mthen\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40m24\u001b[0m\u001b[40m \u001b[0m\u001b[40mh\u001b[0m\u001b[40m \u001b[0m\u001b[40mafter\u001b[0m\u001b[40m \u001b[0m\u001b[40mthe\u001b[0m\u001b[40m \u001b[0m\u001b[40mblood\u001b[0m\u001b[40m \u001b[0m\u001b[40mfeeding\u001b[0m\u001b[40m \u001b[0m\u001b[40m,\u001b[0m\u001b[40m \u001b[0m\u001b[40mgut\u001b[0m\u001b[40m \u001b[0m\u001b[40msections\u001b[0m\u001b[40m \u001b[0m\u001b[40mwere\u001b[0m\u001b[40m \u001b[0m\u001b[40mprepared\u001b[0m\u001b[40m \u001b[0m\u001b[40mfor\u001b[0m\u001b[40m \u001b[0m\u001b[40mhis\u001b[0m\u001b[40mtology\u001b[0m\u001b[40m \u001b[0m\u001b[40mand\u001b[0m\u001b[40m \u001b[0m\u001b[40mstained\u001b[0m\u001b[40m \u001b[0m\u001b[40mwith\u001b[0m\u001b[40m \u001b[0m\u001b[40mhem\u001b[0m\u001b[40mato\u001b[0m\u001b[40mxy\u001b[0m\u001b[40mlin\u001b[0m\u001b[40m \u001b[0m\u001b[40mand\u001b[0m\u001b[40m \u001b[0m\u001b[42me\u001b[0m\u001b[42mosi\u001b[0m\u001b[42mn\u001b[0m\u001b[40m \u001b[0m\u001b[40m(\u001b[0m\u001b[40m \u001b[0m\u001b[41mHE\u001b[0m\u001b[40m \u001b[0m\u001b[40m)\u001b[0m\u001b[40m \u001b[0m\u001b[40m.\u001b[0m\n",
      "Truth:   \u001b[40mMosquitoes \u001b[0m\u001b[40mwere \u001b[0m\u001b[40mallowed \u001b[0m\u001b[40mto \u001b[0m\u001b[40mfeed \u001b[0m\u001b[40mon \u001b[0m\u001b[40ma \u001b[0m\u001b[40mhuman \u001b[0m\u001b[40m, \u001b[0m\u001b[40mthen \u001b[0m\u001b[40m, \u001b[0m\u001b[40m24 \u001b[0m\u001b[40mh \u001b[0m\u001b[40mafter \u001b[0m\u001b[40mthe \u001b[0m\u001b[40mblood \u001b[0m\u001b[40mfeeding \u001b[0m\u001b[40m, \u001b[0m\u001b[40mgut \u001b[0m\u001b[40msections \u001b[0m\u001b[40mwere \u001b[0m\u001b[40mprepared \u001b[0m\u001b[40mfor \u001b[0m\u001b[40mhistology \u001b[0m\u001b[40mand \u001b[0m\u001b[40mstained \u001b[0m\u001b[40mwith \u001b[0m\u001b[42mhematoxylin \u001b[0m\u001b[44mand \u001b[0m\u001b[44meosin \u001b[0m\u001b[40m( \u001b[0m\u001b[41mHE \u001b[0m\u001b[40m) \u001b[0m\u001b[40m. \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words, outputs, truths = choose_multiple()\n",
    "for i in range(len(words)):\n",
    "    vizu(words[i], outputs[i], truths[i], type=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_sentence \u001b[38;5;129;01min\u001b[39;00m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     test_output \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     words \u001b[38;5;241m=\u001b[39m test_sentence\n\u001b[1;32m      8\u001b[0m     truth \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:248\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    246\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/pipelines/base.py:1177\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1174\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1175\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:285\u001b[0m, in \u001b[0;36mTokenClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits,\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m    295\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:1233\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1245\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:822\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m--> 822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:587\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    580\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    581\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m         output_attentions,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:531\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    528\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    532\u001b[0m ffn_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:466\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:469\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 469\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m    471\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(x)\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MScAI/Semester2/NLP/Coursework/code/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true, pred)\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
