{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Back, Style\n",
    "\n",
    "TEXT2ID = {\n",
    "    \"O\": 0,\n",
    "    \"B-AC\": 1,\n",
    "    \"B-LF\": 2,\n",
    "    \"I-LF\": 3,\n",
    "}\n",
    "\n",
    "def vizu(words, output, type=None):\n",
    "    sentence = \" \".join(words)\n",
    "    out_words = []\n",
    "    out_label = []\n",
    "    index = 1\n",
    "    for i in range(len(output)):\n",
    "        start = output[i]['start']\n",
    "        end = output[i]['end']\n",
    "        word = output[i]['word']\n",
    "        if word[0] != '#':\n",
    "            out_words.append(' ')\n",
    "            out_label.append(0)\n",
    "            index += 1\n",
    "        out_words.append(sentence[start:end])\n",
    "        if type==1:\n",
    "            out_label.append(TEXT2ID[output[i]['entity']])\n",
    "        else:\n",
    "            out_label.append(int(output[i]['entity'][-1]))\n",
    "    col = {0: Back.BLACK, 1: Back.RED, 2: Back.GREEN, 3: Back.BLUE, 4: Back.MAGENTA}\n",
    "    for i in range(len(out_words)):\n",
    "        print(col[out_label[i]], end='')\n",
    "        print(out_words[i], end='')\n",
    "        print(Style.RESET_ALL, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
    "TEXT2ID = {\n",
    "    \"B-O\": 0,\n",
    "    \"B-AC\": 1,\n",
    "    \"B-LF\": 2,\n",
    "    \"I-LF\": 3,\n",
    "}\n",
    "datasets = datasets.map(lambda x: {\"ner_tags\": [TEXT2ID[tag] for tag in x[\"ner_tags\"]]})\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if True else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'checkpoints/checkpoint-1200'\n",
    "path = 'surrey-nlp/roberta-base-finetuned-abbr'\n",
    "model = AutoModelForTokenClassification.from_pretrained(path, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\"ner\", model=model, tokenizer=tokenizer, ignore_labels=[])\n",
    "\n",
    "def choose(i=None):\n",
    "    if i is None:\n",
    "        i = torch.randint(0, len(datasets[\"test\"][\"tokens\"]), (1,)).item()\n",
    "    output = pipeline(\" \".join(datasets[\"test\"][\"tokens\"][i]))\n",
    "    words = datasets[\"test\"][\"tokens\"][i]\n",
    "\n",
    "    return words, output\n",
    "\n",
    "def choose_multiple(nb=5):\n",
    "    indices = torch.randint(0, len(datasets[\"test\"][\"tokens\"]), (nb,))\n",
    "    words = []\n",
    "    outputs = []\n",
    "    for i in indices:\n",
    "        w, o = choose(i)\n",
    "        words.append(w)\n",
    "        outputs.append(o)\n",
    "    return words, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'O'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m words, outputs \u001b[38;5;241m=\u001b[39m choose_multiple()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words)):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mvizu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mvizu\u001b[0;34m(words, output, type)\u001b[0m\n\u001b[1;32m     23\u001b[0m out_words\u001b[38;5;241m.\u001b[39mappend(sentence[start:end])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     out_label\u001b[38;5;241m.\u001b[39mappend(\u001b[43mTEXT2ID\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     out_label\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mint\u001b[39m(output[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'O'"
     ]
    }
   ],
   "source": [
    "words, outputs = choose_multiple()\n",
    "for i in range(len(words)):\n",
    "    vizu(words[i], outputs[i], type=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
