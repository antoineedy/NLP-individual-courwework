{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antoine EDY\n",
    "# Natural Language Processing (COMM061) - Coursework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import nltk\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_preprocess(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    # make everything lowercase\n",
    "    #df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [i.lower() for i in x])\n",
    "    # lematize\n",
    "    lematizer = nltk.WordNetLemmatizer()\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [lematizer.lemmatize(i) for i in x])\n",
    "    #stemming\n",
    "    #stemmer = nltk.PorterStemmer()\n",
    "    #df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "    return df\n",
    "\n",
    "train_dataset = the_preprocess(dataset[\"train\"])\n",
    "test_dataset = the_preprocess(dataset[\"test\"])\n",
    "val_dataset = the_preprocess(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "lematizer = nltk.WordNetLemmatizer()\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "sentence = sentence.split()\n",
    "sentence = [lematizer.lemmatize(i) for i in sentence]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "sentence = sentence.split()\n",
    "sentence = [lematizer.lemmatize(i) for i in sentence]\n",
    "sentence = [stemmer.stem(i) for i in sentence]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT2ID: {'B-O': 0, 'B-AC': 1, 'PAD': 2, 'B-LF': 3, 'I-LF': 4}\n",
      "ID2TEXT: {0: 'B-O', 1: 'B-AC', 2: 'PAD', 3: 'B-LF', 4: 'I-LF'}\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1072 entries, 0 to 1071\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tokens     1072 non-null   object\n",
      " 1   labels     1072 non-null   object\n",
      " 2   ids        1072 non-null   object\n",
      " 3   sentences  1072 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 33.6+ KB\n"
     ]
    }
   ],
   "source": [
    "TEXT2ID = {\n",
    "    \"B-O\": 0,\n",
    "    \"B-AC\": 1,\n",
    "    \"PAD\": 2,\n",
    "    \"B-LF\": 3,\n",
    "    \"I-LF\": 4,\n",
    "}\n",
    "ID2TEXT = {v: k for k, v in TEXT2ID.items()}\n",
    "\n",
    "print(f\"TEXT2ID: {TEXT2ID}\\nID2TEXT: {ID2TEXT}\\n\")\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.drop(columns=['pos_tags'])\n",
    "    df = df.rename(columns={\"ner_tags\": \"labels\"})\n",
    "    df[\"ids\"] = df[\"labels\"].apply(lambda x: [TEXT2ID[i] for i in x])\n",
    "    df[\"sentences\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_dataset = preprocess(train_dataset)\n",
    "test_dataset = preprocess(test_dataset)\n",
    "val_dataset = preprocess(val_dataset)\n",
    "\n",
    "train_dataset.info()\n",
    "\n",
    "\n",
    "# Here the exploration to add at the end of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>ids</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[For, this, purpose, the, Gothenburg, Young, P...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...</td>\n",
       "      <td>[0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>For this purpose the Gothenburg Young Persons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, following, physiological, trait, were, m...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>The following physiological trait were measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Minor, H, antigen, alloimmune, response, read...</td>\n",
       "      <td>[B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...</td>\n",
       "      <td>Minor H antigen alloimmune response readily oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[EPI, =, Echo, planar, imaging, .]</td>\n",
       "      <td>[B-AC, B-O, B-LF, I-LF, I-LF, B-O]</td>\n",
       "      <td>[1, 0, 3, 4, 4, 0]</td>\n",
       "      <td>EPI = Echo planar imaging .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Furthermore, ,, eNOS, -, derived, NO, S, -, n...</td>\n",
       "      <td>[B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Furthermore , eNOS - derived NO S - nitrosylat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [For, this, purpose, the, Gothenburg, Young, P...   \n",
       "1  [The, following, physiological, trait, were, m...   \n",
       "2  [Minor, H, antigen, alloimmune, response, read...   \n",
       "3                 [EPI, =, Echo, planar, imaging, .]   \n",
       "4  [Furthermore, ,, eNOS, -, derived, NO, S, -, n...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...   \n",
       "1  [B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...   \n",
       "2  [B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...   \n",
       "3                 [B-AC, B-O, B-LF, I-LF, I-LF, B-O]   \n",
       "4  [B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...   \n",
       "\n",
       "                                                 ids  \\\n",
       "0      [0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...   \n",
       "3                                 [1, 0, 3, 4, 4, 0]   \n",
       "4  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           sentences  \n",
       "0  For this purpose the Gothenburg Young Persons ...  \n",
       "1  The following physiological trait were measure...  \n",
       "2  Minor H antigen alloimmune response readily oc...  \n",
       "3                        EPI = Echo planar imaging .  \n",
       "4  Furthermore , eNOS - derived NO S - nitrosylat...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of labels\n",
    "all_labels = [label for labels in train_dataset[\"labels\"] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 40000\n",
      "Number of unique labels: 4\n",
      "Labels: {'B-LF', 'B-O', 'I-LF', 'B-AC'}\n",
      "Number of B-LF: 1462\n",
      "Number of B-O: 32971\n",
      "Number of I-LF: 3231\n",
      "Number of B-AC: 2336\n"
     ]
    }
   ],
   "source": [
    "# plot the distribution of labels\n",
    "print(f\"Number of labels: {len(all_labels)}\")\n",
    "print(f\"Number of unique labels: {len(set(all_labels))}\")\n",
    "print(f\"Labels: {set(all_labels)}\")\n",
    "\n",
    "for i in set(all_labels):\n",
    "    print(f\"Number of {i}: {all_labels.count(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072\n",
      "126\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>ids</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[For, this, purpose, the, Gothenburg, Young, P...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...</td>\n",
       "      <td>[0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>For this purpose the Gothenburg Young Persons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, following, physiological, trait, were, m...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>The following physiological trait were measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Minor, H, antigen, alloimmune, response, read...</td>\n",
       "      <td>[B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...</td>\n",
       "      <td>Minor H antigen alloimmune response readily oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[EPI, =, Echo, planar, imaging, .]</td>\n",
       "      <td>[B-AC, B-O, B-LF, I-LF, I-LF, B-O]</td>\n",
       "      <td>[1, 0, 3, 4, 4, 0]</td>\n",
       "      <td>EPI = Echo planar imaging .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Furthermore, ,, eNOS, -, derived, NO, S, -, n...</td>\n",
       "      <td>[B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Furthermore , eNOS - derived NO S - nitrosylat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [For, this, purpose, the, Gothenburg, Young, P...   \n",
       "1  [The, following, physiological, trait, were, m...   \n",
       "2  [Minor, H, antigen, alloimmune, response, read...   \n",
       "3                 [EPI, =, Echo, planar, imaging, .]   \n",
       "4  [Furthermore, ,, eNOS, -, derived, NO, S, -, n...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...   \n",
       "1  [B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...   \n",
       "2  [B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...   \n",
       "3                 [B-AC, B-O, B-LF, I-LF, I-LF, B-O]   \n",
       "4  [B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...   \n",
       "\n",
       "                                                 ids  \\\n",
       "0      [0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...   \n",
       "3                                 [1, 0, 3, 4, 4, 0]   \n",
       "4  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           sentences  \n",
       "0  For this purpose the Gothenburg Young Persons ...  \n",
       "1  The following physiological trait were measure...  \n",
       "2  Minor H antigen alloimmune response readily oc...  \n",
       "3                        EPI = Echo planar imaging .  \n",
       "4  Furthermore , eNOS - derived NO S - nitrosylat...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': <torchtext.data.field.Field object at 0x1760d6470>, 'text': <torchtext.data.field.Field object at 0x30bef7e50>}\n",
      "['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'wa', 'developed', '.']\n",
      "['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O']\n",
      "Train: 1072\n",
      "Dev: 126\n",
      "Test: 153\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, Dataset, Example\n",
    "\n",
    "text_field = Field(sequential=True, tokenize=lambda x:x, include_lengths=True) # Default behaviour is to tokenize by splitting\n",
    "label_field = Field(sequential=True, tokenize=lambda x:x, is_target=True)\n",
    "\n",
    "fields = {\n",
    "    'sentences': ('text', text_field),\n",
    "    'ids': ('label', label_field)\n",
    "}\n",
    "\n",
    "def read_data(df):\n",
    "    examples = []\n",
    "    fields = {'sentence_labels': ('labels', label_field),\n",
    "              'sentence_tokens': ('text', text_field)}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        tokens = df['tokens'][i]\n",
    "        labels = df['labels'][i]\n",
    "        \n",
    "        e = Example.fromdict({\"sentence_labels\": labels, \"sentence_tokens\": tokens},\n",
    "                             fields=fields)\n",
    "        examples.append(e)\n",
    "    \n",
    "    return Dataset(examples, fields=[('labels', label_field), ('text', text_field)])\n",
    "\n",
    "\n",
    "train_data = read_data(train_dataset)\n",
    "val_data = read_data(val_dataset)\n",
    "test_data = read_data(test_dataset)\n",
    "\n",
    "print(train_data.fields)\n",
    "print(train_data[0].text)\n",
    "print(train_data[0].labels)\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Dev:\", len(val_data))\n",
    "print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "text_field.build_vocab(train_data, max_size=VOCAB_SIZE)\n",
    "label_field.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_iter = BucketIterator(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                            sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "val_iter = BucketIterator(dataset=val_data, batch_size=BATCH_SIZE, \n",
    "                          sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "test_iter = BucketIterator(dataset=test_data, batch_size=BATCH_SIZE, \n",
    "                           sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained embeddings\n",
      "Initializing embedding matrix\n",
      "torch.Size([8711, 300])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "emb = 'fasttext'\n",
    "\n",
    "if emb == 'fasttext':\n",
    "\n",
    "    EMBEDDING_PATH = \"/Users/antoineedy/Documents/MScAI/Semester2/NLP/Coursework/code/data/cc.en.300.vec\"\n",
    "\n",
    "    def load_embeddings(path):\n",
    "        \"\"\" Load the FastText embeddings from the embedding file. \"\"\"\n",
    "        print(\"Loading pre-trained embeddings\")\n",
    "        \n",
    "        embeddings = {}\n",
    "        with open(path) as i:\n",
    "            for line in i:\n",
    "                if len(line) > 2: \n",
    "                    line = line.strip().split()\n",
    "                    word = line[0]\n",
    "                    embedding = np.array(line[1:])\n",
    "                    embeddings[word] = embedding\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "\n",
    "    def initialize_embeddings(embeddings, vocabulary):\n",
    "        \"\"\" Use the pre-trained embeddings to initialize an embedding matrix. \"\"\"\n",
    "        print(\"Initializing embedding matrix\")\n",
    "        embedding_size = len(embeddings[\".\"])\n",
    "        embedding_matrix = np.zeros((len(vocabulary), embedding_size), dtype=np.float32)\n",
    "                                    \n",
    "        for idx, word in enumerate(vocabulary.itos): \n",
    "            if word in embeddings:\n",
    "                embedding_matrix[idx,:] = embeddings[word]\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embeddings = load_embeddings(EMBEDDING_PATH)\n",
    "    embedding_matrix = initialize_embeddings(embeddings, text_field.vocab)\n",
    "    embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "elif emb == 'glove':\n",
    "\n",
    "    EMBEDDING_PATH = \"data/glove.6B.300d.txt\"\n",
    "\n",
    "    def load_embeddings(path):\n",
    "        \"\"\" Load the FastText embeddings from the embedding file. \"\"\"\n",
    "        print(\"Loading pre-trained embeddings\")\n",
    "        \n",
    "        embeddings = {}\n",
    "        with open(path) as i:\n",
    "            for line in i:\n",
    "                if len(line) > 2: \n",
    "                    line = line.strip().split()\n",
    "                    word = line[0]\n",
    "                    embedding = np.array(line[1:])\n",
    "                    embeddings[word] = embedding\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "\n",
    "    def initialize_embeddings(embeddings, vocabulary):\n",
    "        \"\"\" Use the pre-trained embeddings to initialize an embedding matrix. \"\"\"\n",
    "        print(\"Initializing embedding matrix\")\n",
    "        embedding_size = len(embeddings[\".\"])\n",
    "        embedding_matrix = np.zeros((len(vocabulary), embedding_size), dtype=np.float32)\n",
    "                                    \n",
    "        for idx, word in enumerate(vocabulary.itos): \n",
    "            if word in embeddings:\n",
    "                embedding_matrix[idx,:] = embeddings[word]\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embeddings = load_embeddings(EMBEDDING_PATH)\n",
    "    embedding_matrix = initialize_embeddings(embeddings, text_field.vocab)\n",
    "    embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "\n",
    "elif emb == 'word2vec':\n",
    "    import gensim\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    em = []\n",
    "    for word in text_field.vocab.itos:\n",
    "        if word in model:\n",
    "            em.append(model.get_vector(word))\n",
    "        else:\n",
    "            em.append(np.zeros(300))\n",
    "    em = np.array(em)\n",
    "    embedding_matrix = torch.tensor(em, dtype=torch.float32)\n",
    "    print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size, embeddings=None):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        if embeddings is None:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=1)\n",
    "        \n",
    "        # 3. Optional dropout layer\n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "\n",
    "        # 4. Dense Layer\n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, batch_text, batch_lengths):\n",
    "\n",
    "        embeddings = self.embeddings(batch_text)\n",
    "        \n",
    "        packed_seqs = pack_padded_sequence(embeddings, batch_lengths)\n",
    "        lstm_output, _ = self.lstm(packed_seqs)\n",
    "        lstm_output, _ = pad_packed_sequence(lstm_output)\n",
    "        lstm_output = self.dropout_layer(lstm_output)\n",
    "        \n",
    "        logits = self.hidden2tag(lstm_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6: ['<unk>', '<pad>', 'B-O', 'I-LF', 'B-AC', 'B-LF']\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from seqeval.metrics import f1_score as seqeval_f1_score\n",
    "\n",
    "def remove_predictions_for_masked_items(predicted_labels, correct_labels): \n",
    "\n",
    "    predicted_labels_without_mask = []\n",
    "    correct_labels_without_mask = []\n",
    "        \n",
    "    for p, c in zip(predicted_labels, correct_labels):\n",
    "        if c > 1:\n",
    "            predicted_labels_without_mask.append(p)\n",
    "            correct_labels_without_mask.append(c)\n",
    "            \n",
    "    return predicted_labels_without_mask, correct_labels_without_mask\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_CLASSES = len(label_field.vocab)\n",
    "print(f\"Number of classes: {NUM_CLASSES}: {label_field.vocab.itos}\")\n",
    "\n",
    "def train(model, train_iter, dev_iter, batch_size, max_epochs, num_batches, patience, output_path):\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # add weight to indexes 3, 4, 5\n",
    "    w = [0, 0, 0.0443, 0.6259, 1.0000, 0.4525]\n",
    "    class_weights = torch.tensor(w).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight = class_weights, ignore_index=1)  # we mask the <pad> labels\n",
    "    # Hinge loss\n",
    "    # criterion = nn.MultiMarginLoss(margin=1.0, weight=class_weights, reduction='mean')\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # SGD\n",
    "    #lr = 0.1\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # AdamW\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "    train_f_score_history = []\n",
    "    dev_f_score_history = []\n",
    "    no_improvement = 0\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            text_length, cur_batch_size = batch.text[0].shape\n",
    "            \n",
    "            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n",
    "            gold = batch.labels.to(device).view(cur_batch_size*text_length)\n",
    "            \n",
    "            loss = criterion(pred, gold)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            \n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "            \n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "            \n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "\n",
    "        train_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n",
    "        train_f_score_history.append(train_scores[2])\n",
    "            \n",
    "        print(\"Total training loss:\", total_loss)\n",
    "        print(\"Training performance:\", train_scores)\n",
    "\n",
    "        #tensorboard\n",
    "        writer.add_scalar('train/loss', total_loss, epoch)\n",
    "        writer.add_scalar('train/precision', train_scores[2], epoch)\n",
    "\n",
    "        \n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        for batch in dev_iter:\n",
    "\n",
    "            text_length, cur_batch_size = batch.text[0].shape\n",
    "\n",
    "            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n",
    "            gold = batch.labels.to(device).view(cur_batch_size * text_length)\n",
    "            loss = criterion(pred, gold)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "            \n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "            \n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "\n",
    "        dev_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n",
    "            \n",
    "        print(\"Total development loss:\", total_loss)\n",
    "        print(\"Development performance:\", dev_scores)\n",
    "\n",
    "        writer.add_scalar('val/loss', total_loss, epoch)\n",
    "        writer.add_scalar('val/precision', dev_scores[2], epoch)\n",
    "\n",
    "\n",
    "        labels = label_field.vocab.itos[2:]\n",
    "        labels = sorted(labels, key=lambda x: x.split(\"-\")[-1])\n",
    "        label_idxs = [label_field.vocab.stoi[l] for l in labels]\n",
    "\n",
    "        cr = classification_report(correct, predictions, labels = label_idxs, target_names=labels, output_dict=True)\n",
    "\n",
    "        out = {}\n",
    "        for key in cr.keys():\n",
    "            if key == 'accuracy':\n",
    "                out[key] = cr[key]\n",
    "            else:\n",
    "                for new_k in ['precision', 'recall', 'f1-score']:\n",
    "                    out[key+'_'+new_k] = cr[key][new_k]\n",
    "        \n",
    "        for (key, value) in out.items():\n",
    "            writer.add_scalar(f'test/{key}', value, epoch)\n",
    "\n",
    "        t2id = ['<unk>', '<pad>', 'B-O', 'I-LF', 'B-AC', 'B-LF']\n",
    "        t2id= {i: t2id[i] for i in range(len(t2id))}\n",
    "\n",
    "\n",
    "        correct = [t2id[i] for i in correct]\n",
    "        predictions = [t2id[i] for i in predictions]\n",
    "\n",
    "        # sequeval f1 score\n",
    "        seqeval_f1 = seqeval_f1_score([correct], [predictions])\n",
    "        print(seqeval_f1)\n",
    "        writer.add_scalar('test/seqeval_f1', seqeval_f1, epoch)\n",
    "        \n",
    "        \n",
    "        dev_f = dev_scores[2]\n",
    "\n",
    "        dev_f = out['macro avg_f1-score']\n",
    "\n",
    "        if len(dev_f_score_history) > patience and dev_f < max(dev_f_score_history):\n",
    "            no_improvement += 1\n",
    "\n",
    "        elif len(dev_f_score_history) == 0 or dev_f > max(dev_f_score_history):\n",
    "            print(\"Saving model.\")\n",
    "            torch.save(model, output_path)\n",
    "            no_improvement = 0\n",
    "            \n",
    "        if no_improvement > patience:\n",
    "            print(\"Macro average F1-score does not improve anymore. Stop training.\")\n",
    "            dev_f_score_history.append(dev_f)\n",
    "            break\n",
    "            \n",
    "        dev_f_score_history.append(dev_f)\n",
    "        \n",
    "    return train_f_score_history, dev_f_score_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_iter, batch_size, labels, target_names): \n",
    "    \n",
    "    total_loss = 0\n",
    "    predictions, correct = [], []\n",
    "    for batch in test_iter:\n",
    "\n",
    "        text_length, cur_batch_size = batch.text[0].shape\n",
    "\n",
    "        pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n",
    "        gold = batch.labels.to(device).view(cur_batch_size * text_length)\n",
    "\n",
    "        _, pred_indices = torch.max(pred, 1)\n",
    "        predicted_labels = list(pred_indices.cpu().numpy())\n",
    "        correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "\n",
    "        predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, correct_labels)\n",
    "\n",
    "        predictions += predicted_labels\n",
    "        correct += correct_labels\n",
    "    \n",
    "    print(classification_report(correct, predictions, labels=labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6 : ['<unk>', '<pad>', 'B-O', 'I-LF', 'B-AC', 'B-LF']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 34/34 [00:05<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 41.681060552597046\n",
      "Training performance: (0.267525, 0.267525, 0.267525, None)\n",
      "Total development loss: 3.8846705555915833\n",
      "Development performance: (0.6978, 0.6978, 0.6978, None)\n",
      "0.7053000618684264\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 34/34 [00:05<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 22.121582567691803\n",
      "Training performance: (0.6881, 0.6881, 0.6881, None)\n",
      "Total development loss: 2.5240572690963745\n",
      "Development performance: (0.7964, 0.7964, 0.7964, None)\n",
      "0.8009728415079044\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 34/34 [00:05<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 16.462463855743408\n",
      "Training performance: (0.780825, 0.780825, 0.780825, None)\n",
      "Total development loss: 2.3635749220848083\n",
      "Development performance: (0.757, 0.757, 0.757, None)\n",
      "0.7624138994551249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 34/34 [00:07<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 14.2469382584095\n",
      "Training performance: (0.7912, 0.7912, 0.7912, None)\n",
      "Total development loss: 2.141958564519882\n",
      "Development performance: (0.8062, 0.8062, 0.8062, None)\n",
      "0.8092238927265339\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 34/34 [00:05<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 12.867154031991959\n",
      "Training performance: (0.8057, 0.8057, 0.8057, None)\n",
      "Total development loss: 2.0472545623779297\n",
      "Development performance: (0.8118, 0.8118, 0.8118, None)\n",
      "0.8142623620760114\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 34/34 [00:05<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 11.622651547193527\n",
      "Training performance: (0.825875, 0.825875, 0.825875, None)\n",
      "Total development loss: 2.1740987598896027\n",
      "Development performance: (0.8702, 0.8702, 0.8702, None)\n",
      "0.8727162612294338\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 34/34 [00:05<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 10.712860822677612\n",
      "Training performance: (0.832475, 0.832475, 0.832475, None)\n",
      "Total development loss: 1.8778501451015472\n",
      "Development performance: (0.8298, 0.8298, 0.8298, None)\n",
      "0.8308255388369256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 34/34 [00:05<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 10.20322473347187\n",
      "Training performance: (0.830925, 0.830925, 0.830925, None)\n",
      "Total development loss: 2.134759098291397\n",
      "Development performance: (0.8946, 0.8946, 0.8946, None)\n",
      "0.8965795580667945\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 34/34 [00:05<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 10.04070945084095\n",
      "Training performance: (0.845075, 0.845075, 0.845075, None)\n",
      "Total development loss: 1.87765833735466\n",
      "Development performance: (0.8356, 0.8356, 0.8356, None)\n",
      "0.8386965790275099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 34/34 [00:05<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 8.898783683776855\n",
      "Training performance: (0.854175, 0.854175, 0.854175, None)\n",
      "Total development loss: 1.8767363131046295\n",
      "Development performance: (0.851, 0.851, 0.851, None)\n",
      "0.8529591214154972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 34/34 [00:05<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 8.353291928768158\n",
      "Training performance: (0.85805, 0.85805, 0.85805, None)\n",
      "Total development loss: 1.893040508031845\n",
      "Development performance: (0.87, 0.87, 0.87, None)\n",
      "0.8733677497722443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 34/34 [00:05<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 7.805068880319595\n",
      "Training performance: (0.86765, 0.86765, 0.86765, None)\n",
      "Total development loss: 1.7988835871219635\n",
      "Development performance: (0.8424, 0.8424, 0.8424, None)\n",
      "0.8460758466842424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 34/34 [00:05<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 7.358078688383102\n",
      "Training performance: (0.872425, 0.872425, 0.872425, None)\n",
      "Total development loss: 1.9349627494812012\n",
      "Development performance: (0.8064, 0.8064, 0.8064, None)\n",
      "0.8098598752173468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 34/34 [00:06<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 7.022028848528862\n",
      "Training performance: (0.874025, 0.874025, 0.874025, None)\n",
      "Total development loss: 1.7299894988536835\n",
      "Development performance: (0.826, 0.826, 0.826, None)\n",
      "0.827998379254457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 34/34 [00:05<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 6.384292080998421\n",
      "Training performance: (0.887375, 0.887375, 0.887375, None)\n",
      "Total development loss: 1.8119201064109802\n",
      "Development performance: (0.8604, 0.8604, 0.8604, None)\n",
      "0.8629832017810161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 34/34 [00:05<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 6.071491666138172\n",
      "Training performance: (0.893075, 0.893075, 0.893075, None)\n",
      "Total development loss: 1.7423635721206665\n",
      "Development performance: (0.8474, 0.8474, 0.8474, None)\n",
      "0.8489820723184441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 34/34 [00:06<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 5.6805156618356705\n",
      "Training performance: (0.897575, 0.897575, 0.897575, None)\n",
      "Total development loss: 1.894538700580597\n",
      "Development performance: (0.8796, 0.8796, 0.8796, None)\n",
      "0.8830196316535114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 34/34 [00:05<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 5.1753774508833885\n",
      "Training performance: (0.9026, 0.9026, 0.9026, None)\n",
      "Total development loss: 1.898715317249298\n",
      "Development performance: (0.8634, 0.8634, 0.8634, None)\n",
      "0.8657079197893458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 34/34 [00:06<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.973257876932621\n",
      "Training performance: (0.907875, 0.907875, 0.907875, None)\n",
      "Total development loss: 2.24480402469635\n",
      "Development performance: (0.88, 0.88, 0.88, None)\n",
      "0.8824959481361426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 34/34 [00:05<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.727583073079586\n",
      "Training performance: (0.91275, 0.91275, 0.91275, None)\n",
      "Total development loss: 2.075860410928726\n",
      "Development performance: (0.8606, 0.8606, 0.8606, None)\n",
      "0.8621248986212491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 34/34 [00:04<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.125961758196354\n",
      "Training performance: (0.918225, 0.918225, 0.918225, None)\n",
      "Total development loss: 2.140803635120392\n",
      "Development performance: (0.8494, 0.8494, 0.8494, None)\n",
      "0.852449041679343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 34/34 [00:05<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 3.8590009212493896\n",
      "Training performance: (0.922475, 0.922475, 0.922475, None)\n",
      "Total development loss: 2.3291521966457367\n",
      "Development performance: (0.8928, 0.8928, 0.8928, None)\n",
      "0.8955977382875606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 34/34 [00:05<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 3.4817947670817375\n",
      "Training performance: (0.9316, 0.9316, 0.9316, None)\n",
      "Total development loss: 2.521644800901413\n",
      "Development performance: (0.905, 0.905, 0.905, None)\n",
      "0.9070331447049313\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 34/34 [00:05<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 3.1848374903202057\n",
      "Training performance: (0.934, 0.934, 0.934, None)\n",
      "Total development loss: 2.379627048969269\n",
      "Development performance: (0.8892, 0.8892, 0.8892, None)\n",
      "0.8902611864749949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 34/34 [00:04<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 3.0508559085428715\n",
      "Training performance: (0.941075, 0.941075, 0.941075, None)\n",
      "Total development loss: 2.2736799120903015\n",
      "Development performance: (0.8856, 0.8856, 0.8856, None)\n",
      "0.8875429032909349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 34/34 [00:05<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.7295382246375084\n",
      "Training performance: (0.940725, 0.940725, 0.940725, None)\n",
      "Total development loss: 2.1034477949142456\n",
      "Development performance: (0.8556, 0.8556, 0.8556, None)\n",
      "0.8568534089757877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 34/34 [00:04<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.62605844438076\n",
      "Training performance: (0.939125, 0.939125, 0.939125, None)\n",
      "Total development loss: 2.424968719482422\n",
      "Development performance: (0.8904, 0.8904, 0.8904, None)\n",
      "0.8916927425052992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 34/34 [00:04<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.4763612151145935\n",
      "Training performance: (0.9462, 0.9462, 0.9462, None)\n",
      "Total development loss: 2.4856720864772797\n",
      "Development performance: (0.8798, 0.8798, 0.8798, None)\n",
      "0.8826623507991099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 34/34 [00:04<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.2237343303859234\n",
      "Training performance: (0.94985, 0.94985, 0.94985, None)\n",
      "Total development loss: 2.293545812368393\n",
      "Development performance: (0.8702, 0.8702, 0.8702, None)\n",
      "0.8713231577883352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 34/34 [00:04<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.255358813330531\n",
      "Training performance: (0.95, 0.95, 0.95, None)\n",
      "Total development loss: 3.0530121326446533\n",
      "Development performance: (0.9112, 0.9112, 0.9112, None)\n",
      "0.9132189707366297\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 34/34 [00:04<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.982375379651785\n",
      "Training performance: (0.955275, 0.955275, 0.955275, None)\n",
      "Total development loss: 3.0594310760498047\n",
      "Development performance: (0.9, 0.9, 0.9, None)\n",
      "0.9029714978775015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 34/34 [00:04<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.7690251395106316\n",
      "Training performance: (0.959925, 0.959925, 0.959925, None)\n",
      "Total development loss: 2.834263861179352\n",
      "Development performance: (0.8988, 0.8988, 0.8988, None)\n",
      "0.9012145748987854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 34/34 [00:04<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.8423053491860628\n",
      "Training performance: (0.953125, 0.953125, 0.953125, None)\n",
      "Total development loss: 3.0929524302482605\n",
      "Development performance: (0.9022, 0.9022, 0.9022, None)\n",
      "0.905770782889427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 34/34 [00:04<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.803859457373619\n",
      "Training performance: (0.957775, 0.957775, 0.957775, None)\n",
      "Total development loss: 2.871930181980133\n",
      "Development performance: (0.8904, 0.8904, 0.8904, None)\n",
      "0.8932215375290433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 34/34 [00:04<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.7904788386076689\n",
      "Training performance: (0.957225, 0.957225, 0.957225, None)\n",
      "Total development loss: 2.9139504730701447\n",
      "Development performance: (0.9042, 0.9042, 0.9042, None)\n",
      "0.9077155824508321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 34/34 [00:04<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.5050229150801897\n",
      "Training performance: (0.9638, 0.9638, 0.9638, None)\n",
      "Total development loss: 3.0296865105628967\n",
      "Development performance: (0.895, 0.895, 0.895, None)\n",
      "0.8971792538671519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 34/34 [00:04<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.3479828229174018\n",
      "Training performance: (0.967025, 0.967025, 0.967025, None)\n",
      "Total development loss: 3.9942741990089417\n",
      "Development performance: (0.911, 0.911, 0.911, None)\n",
      "0.912029087970912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 34/34 [00:05<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.161216538399458\n",
      "Training performance: (0.9731, 0.9731, 0.9731, None)\n",
      "Total development loss: 3.509943664073944\n",
      "Development performance: (0.9004, 0.9004, 0.9004, None)\n",
      "0.9018894614529656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 34/34 [00:05<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.0399597361683846\n",
      "Training performance: (0.974175, 0.974175, 0.974175, None)\n",
      "Total development loss: 3.930672287940979\n",
      "Development performance: (0.9062, 0.9062, 0.9062, None)\n",
      "0.9076426698931236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 34/34 [00:05<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.902461557649076\n",
      "Training performance: (0.977425, 0.977425, 0.977425, None)\n",
      "Total development loss: 4.138614535331726\n",
      "Development performance: (0.9102, 0.9102, 0.9102, None)\n",
      "0.9106674732809034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 34/34 [00:05<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.8685055654495955\n",
      "Training performance: (0.9785, 0.9785, 0.9785, None)\n",
      "Total development loss: 3.9392729997634888\n",
      "Development performance: (0.9046, 0.9046, 0.9046, None)\n",
      "0.9076224129227664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 34/34 [00:05<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.7333069071173668\n",
      "Training performance: (0.982, 0.982, 0.982, None)\n",
      "Total development loss: 4.242374420166016\n",
      "Development performance: (0.9038, 0.9038, 0.9038, None)\n",
      "0.9069814366424536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 34/34 [00:06<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.8985772393643856\n",
      "Training performance: (0.9771, 0.9771, 0.9771, None)\n",
      "Total development loss: 3.7183289527893066\n",
      "Development performance: (0.9042, 0.9042, 0.9042, None)\n",
      "0.9061932620536616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 34/34 [00:05<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.2322655683383346\n",
      "Training performance: (0.948175, 0.948175, 0.948175, None)\n",
      "Total development loss: 2.7940717935562134\n",
      "Development performance: (0.884, 0.884, 0.884, None)\n",
      "0.884483107424641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 34/34 [00:05<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.53855412453413\n",
      "Training performance: (0.96485, 0.96485, 0.96485, None)\n",
      "Total development loss: 3.3213562965393066\n",
      "Development performance: (0.8954, 0.8954, 0.8954, None)\n",
      "0.8969844161100992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 34/34 [00:05<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.056081397458911\n",
      "Training performance: (0.97255, 0.97255, 0.97255, None)\n",
      "Total development loss: 4.108601152896881\n",
      "Development performance: (0.9038, 0.9038, 0.9038, None)\n",
      "0.9052950075642964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 34/34 [00:05<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.7943324223160744\n",
      "Training performance: (0.9803, 0.9803, 0.9803, None)\n",
      "Total development loss: 3.981302499771118\n",
      "Development performance: (0.9012, 0.9012, 0.9012, None)\n",
      "0.9028225806451613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 34/34 [00:05<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.6271514459513128\n",
      "Training performance: (0.9836, 0.9836, 0.9836, None)\n",
      "Total development loss: 4.522134482860565\n",
      "Development performance: (0.9078, 0.9078, 0.9078, None)\n",
      "0.9093844601412714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 34/34 [00:05<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5318172206170857\n",
      "Training performance: (0.986475, 0.986475, 0.986475, None)\n",
      "Total development loss: 4.485099673271179\n",
      "Development performance: (0.9078, 0.9078, 0.9078, None)\n",
      "0.9101917255297679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 34/34 [00:05<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5599563820287585\n",
      "Training performance: (0.98595, 0.98595, 0.98595, None)\n",
      "Total development loss: 4.581151068210602\n",
      "Development performance: (0.9038, 0.9038, 0.9038, None)\n",
      "0.9055387713997985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 34/34 [00:05<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5244255368597806\n",
      "Training performance: (0.9865, 0.9865, 0.9865, None)\n",
      "Total development loss: 5.566702365875244\n",
      "Development performance: (0.91, 0.91, 0.91, None)\n",
      "0.9110169491525425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 34/34 [00:06<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3999288387130946\n",
      "Training performance: (0.99025, 0.99025, 0.99025, None)\n",
      "Total development loss: 5.788962960243225\n",
      "Development performance: (0.9146, 0.9146, 0.9146, None)\n",
      "0.9156092648539779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 34/34 [00:05<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3771852166391909\n",
      "Training performance: (0.990175, 0.990175, 0.990175, None)\n",
      "Total development loss: 5.165010035037994\n",
      "Development performance: (0.9126, 0.9126, 0.9126, None)\n",
      "0.9137045614741718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 34/34 [00:05<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.382469582837075\n",
      "Training performance: (0.990125, 0.990125, 0.990125, None)\n",
      "Total development loss: 5.470381498336792\n",
      "Development performance: (0.9132, 0.9132, 0.9132, None)\n",
      "0.9143087302386467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 34/34 [00:05<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3452336355112493\n",
      "Training performance: (0.99085, 0.99085, 0.99085, None)\n",
      "Total development loss: 4.9055880308151245\n",
      "Development performance: (0.908, 0.908, 0.908, None)\n",
      "0.9089624545821557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 34/34 [00:05<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.44275091402232647\n",
      "Training performance: (0.98805, 0.98805, 0.98805, None)\n",
      "Total development loss: 5.907941460609436\n",
      "Development performance: (0.905, 0.905, 0.905, None)\n",
      "0.9068360556563823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 34/34 [00:05<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.43517217598855495\n",
      "Training performance: (0.9882, 0.9882, 0.9882, None)\n",
      "Total development loss: 5.26470410823822\n",
      "Development performance: (0.907, 0.907, 0.907, None)\n",
      "0.9080645161290323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 34/34 [00:06<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3767853102181107\n",
      "Training performance: (0.99085, 0.99085, 0.99085, None)\n",
      "Total development loss: 5.03219598531723\n",
      "Development performance: (0.9074, 0.9074, 0.9074, None)\n",
      "0.9085587404117884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 34/34 [00:05<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3211774812079966\n",
      "Training performance: (0.992425, 0.992425, 0.992425, None)\n",
      "Total development loss: 5.590544104576111\n",
      "Development performance: (0.9062, 0.9062, 0.9062, None)\n",
      "0.9081375415952405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 34/34 [00:05<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.2727521671913564\n",
      "Training performance: (0.993675, 0.993675, 0.993675, None)\n",
      "Total development loss: 5.488704442977905\n",
      "Development performance: (0.905, 0.905, 0.905, None)\n",
      "0.9067822231180086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 34/34 [00:05<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.2981734818313271\n",
      "Training performance: (0.993125, 0.993125, 0.993125, None)\n",
      "Total development loss: 5.104112863540649\n",
      "Development performance: (0.902, 0.902, 0.902, None)\n",
      "0.9045886395795433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 34/34 [00:05<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.2509565588552505\n",
      "Training performance: (0.993475, 0.993475, 0.993475, None)\n",
      "Total development loss: 6.2971718311309814\n",
      "Development performance: (0.9068, 0.9068, 0.9068, None)\n",
      "0.9084492841298649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 34/34 [00:05<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.2605000617913902\n",
      "Training performance: (0.993925, 0.993925, 0.993925, None)\n",
      "Total development loss: 5.944259762763977\n",
      "Development performance: (0.901, 0.901, 0.901, None)\n",
      "0.9045023218251564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 34/34 [00:05<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.29983419040217996\n",
      "Training performance: (0.99245, 0.99245, 0.99245, None)\n",
      "Total development loss: 5.39612889289856\n",
      "Development performance: (0.9086, 0.9086, 0.9086, None)\n",
      "0.9103030303030304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 34/34 [00:05<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3012443317566067\n",
      "Training performance: (0.992525, 0.992525, 0.992525, None)\n",
      "Total development loss: 5.860893368721008\n",
      "Development performance: (0.9106, 0.9106, 0.9106, None)\n",
      "0.9121676067687349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 34/34 [00:05<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.4282491279300302\n",
      "Training performance: (0.98945, 0.98945, 0.98945, None)\n",
      "Total development loss: 5.188499808311462\n",
      "Development performance: (0.9072, 0.9072, 0.9072, None)\n",
      "0.9089076985086659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 34/34 [00:05<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.44542896980419755\n",
      "Training performance: (0.9886, 0.9886, 0.9886, None)\n",
      "Total development loss: 6.396761775016785\n",
      "Development performance: (0.9144, 0.9144, 0.9144, None)\n",
      "0.9168598206187645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 34/34 [00:05<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.47895533591508865\n",
      "Training performance: (0.98795, 0.98795, 0.98795, None)\n",
      "Total development loss: 5.250759124755859\n",
      "Development performance: (0.9164, 0.9164, 0.9164, None)\n",
      "0.9189189189189191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 34/34 [00:05<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.48748725978657603\n",
      "Training performance: (0.989125, 0.989125, 0.989125, None)\n",
      "Total development loss: 5.198781728744507\n",
      "Development performance: (0.9148, 0.9148, 0.9148, None)\n",
      "0.9172038678485093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 34/34 [00:05<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3792910340707749\n",
      "Training performance: (0.9903, 0.9903, 0.9903, None)\n",
      "Total development loss: 5.463191032409668\n",
      "Development performance: (0.9144, 0.9144, 0.9144, None)\n",
      "0.9156821165303443\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "EMBEDDING_DIM = 300 #fasttext & word2vec & glove\n",
    "# EMBEDDING_DIM = 768 #bert\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = len(label_field.vocab)\n",
    "print(f\"Number of classes: {NUM_CLASSES} : {label_field.vocab.itos}\")\n",
    "MAX_EPOCHS = 70\n",
    "PATIENCE = 50\n",
    "OUTPUT_PATH = \"model_saves/bilstmtagger\"\n",
    "num_batches = math.ceil(len(train_data) / BATCH_SIZE)\n",
    "\n",
    "tagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES, embeddings=embedding_matrix)  # embeddings\n",
    "# tagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES)  # no embeddings\n",
    "\n",
    "train_f, dev_f = train(tagger.to(device), train_iter, val_iter, BATCH_SIZE, MAX_EPOCHS, \n",
    "                       num_batches, PATIENCE, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "df = pd.DataFrame({'epochs': range(0,len(train_f)), \n",
    "                  'train_f': train_f, \n",
    "                   'val_f': dev_f})\n",
    " \n",
    "# multiple line plot\n",
    "plt.plot('epochs', 'train_f', data=df, color='blue', linewidth=2)\n",
    "plt.plot('epochs', 'val_f', data=df, color='green', linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"model_saves/bilstmtagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger(\n",
       "  (embeddings): Embedding(8711, 300)\n",
       "  (lstm): LSTM(300, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = torch.load(OUTPUT_PATH)\n",
    "tagger.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AC       0.58      0.69      0.63       270\n",
      "        I-LF       0.65      0.81      0.72       288\n",
      "        B-LF       0.56      0.63      0.59       150\n",
      "         B-O       0.97      0.93      0.95      4292\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.69      0.76      0.72      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = label_field.vocab.itos[2:]\n",
    "labels = sorted(labels, key=lambda x: x.split(\"-\")[-1])\n",
    "label_idxs = [label_field.vocab.stoi[l] for l in labels]\n",
    "\n",
    "test(tagger, test_iter, BATCH_SIZE, labels = label_idxs, target_names = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Back, Style\n",
    "\n",
    "def vizu(words, output, truth):\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.squeeze().tolist()\n",
    "    col = {0: Back.GREEN, 1: Back.RED, 2: Back.BLACK, 3: Back.BLUE, 4: Back.MAGENTA}\n",
    "    colors1 = [col[i] for i in output]\n",
    "    colors2 = [col[i] for i in truth]\n",
    "    words = [word.replace(\"Ġ\", \"\") for word in words]\n",
    "    print(Style.RESET_ALL + \"Output:\")\n",
    "    for i, word in enumerate(words):\n",
    "        print(colors1[i] + word, end=\" \")\n",
    "    print(Style.RESET_ALL + \"\\nTruth:\")\n",
    "    for i, word in enumerate(words):\n",
    "        print(colors2[i] + word, end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
