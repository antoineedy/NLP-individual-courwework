{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antoine EDY\n",
    "# Natural Language Processing (COMM061) - Coursework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import nltk\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_preprocess(df):\n",
    "    # make everything lowercase\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [i.lower() for i in x])\n",
    "    # lematize\n",
    "    #lematizer = nltk.WordNetLemmatizer()\n",
    "    #df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [lematizer.lemmatize(i) for i in x])\n",
    "    #stemming\n",
    "    #stemmer = nltk.PorterStemmer()\n",
    "    #df[\"tokens\"] = df[\"tokens\"].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "\n",
    "    return df\n",
    "\n",
    "train_dataset = the_preprocess(dataset[\"train\"])\n",
    "test_dataset = the_preprocess(dataset[\"test\"])\n",
    "val_dataset = the_preprocess(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT2ID: {'B-O': 0, 'B-AC': 1, 'PAD': 2, 'B-LF': 3, 'I-LF': 4}\n",
      "ID2TEXT: {0: 'B-O', 1: 'B-AC', 2: 'PAD', 3: 'B-LF', 4: 'I-LF'}\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1072 entries, 0 to 1071\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tokens     1072 non-null   object\n",
      " 1   labels     1072 non-null   object\n",
      " 2   ids        1072 non-null   object\n",
      " 3   sentences  1072 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 33.6+ KB\n"
     ]
    }
   ],
   "source": [
    "TEXT2ID = {\n",
    "    \"B-O\": 0,\n",
    "    \"B-AC\": 1,\n",
    "    \"PAD\": 2,\n",
    "    \"B-LF\": 3,\n",
    "    \"I-LF\": 4,\n",
    "}\n",
    "ID2TEXT = {v: k for k, v in TEXT2ID.items()}\n",
    "\n",
    "print(f\"TEXT2ID: {TEXT2ID}\\nID2TEXT: {ID2TEXT}\\n\")\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.drop(columns=['pos_tags'])\n",
    "    df = df.rename(columns={\"ner_tags\": \"labels\"})\n",
    "    df[\"ids\"] = df[\"labels\"].apply(lambda x: [TEXT2ID[i] for i in x])\n",
    "    df[\"sentences\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_dataset = preprocess(train_dataset)\n",
    "test_dataset = preprocess(test_dataset)\n",
    "val_dataset = preprocess(val_dataset)\n",
    "\n",
    "train_dataset.info()\n",
    "\n",
    "\n",
    "# Here the exploration to add at the end of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>ids</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[For, this, purpose, the, Gothenburg, Young, P...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...</td>\n",
       "      <td>[0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>For this purpose the Gothenburg Young Persons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, following, physiological, traits, were, ...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>The following physiological traits were measur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Minor, H, antigen, alloimmune, responses, rea...</td>\n",
       "      <td>[B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...</td>\n",
       "      <td>Minor H antigen alloimmune responses readily o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[EPI, =, Echo, planar, imaging, .]</td>\n",
       "      <td>[B-AC, B-O, B-LF, I-LF, I-LF, B-O]</td>\n",
       "      <td>[1, 0, 3, 4, 4, 0]</td>\n",
       "      <td>EPI = Echo planar imaging .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Furthermore, ,, eNOS, -, derived, NO, S, -, n...</td>\n",
       "      <td>[B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Furthermore , eNOS - derived NO S - nitrosylat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [For, this, purpose, the, Gothenburg, Young, P...   \n",
       "1  [The, following, physiological, traits, were, ...   \n",
       "2  [Minor, H, antigen, alloimmune, responses, rea...   \n",
       "3                 [EPI, =, Echo, planar, imaging, .]   \n",
       "4  [Furthermore, ,, eNOS, -, derived, NO, S, -, n...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...   \n",
       "1  [B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...   \n",
       "2  [B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...   \n",
       "3                 [B-AC, B-O, B-LF, I-LF, I-LF, B-O]   \n",
       "4  [B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...   \n",
       "\n",
       "                                                 ids  \\\n",
       "0      [0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...   \n",
       "3                                 [1, 0, 3, 4, 4, 0]   \n",
       "4  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           sentences  \n",
       "0  For this purpose the Gothenburg Young Persons ...  \n",
       "1  The following physiological traits were measur...  \n",
       "2  Minor H antigen alloimmune responses readily o...  \n",
       "3                        EPI = Echo planar imaging .  \n",
       "4  Furthermore , eNOS - derived NO S - nitrosylat...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072\n",
      "126\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>ids</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[For, this, purpose, the, Gothenburg, Young, P...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...</td>\n",
       "      <td>[0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>For this purpose the Gothenburg Young Persons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, following, physiological, traits, were, ...</td>\n",
       "      <td>[B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>The following physiological traits were measur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Minor, H, antigen, alloimmune, responses, rea...</td>\n",
       "      <td>[B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...</td>\n",
       "      <td>Minor H antigen alloimmune responses readily o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[EPI, =, Echo, planar, imaging, .]</td>\n",
       "      <td>[B-AC, B-O, B-LF, I-LF, I-LF, B-O]</td>\n",
       "      <td>[1, 0, 3, 4, 4, 0]</td>\n",
       "      <td>EPI = Echo planar imaging .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Furthermore, ,, eNOS, -, derived, NO, S, -, n...</td>\n",
       "      <td>[B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>Furthermore , eNOS - derived NO S - nitrosylat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [For, this, purpose, the, Gothenburg, Young, P...   \n",
       "1  [The, following, physiological, traits, were, ...   \n",
       "2  [Minor, H, antigen, alloimmune, responses, rea...   \n",
       "3                 [EPI, =, Echo, planar, imaging, .]   \n",
       "4  [Furthermore, ,, eNOS, -, derived, NO, S, -, n...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [B-O, B-O, B-O, B-O, B-LF, I-LF, I-LF, I-LF, I...   \n",
       "1  [B-O, B-O, B-O, B-O, B-O, B-O, B-O, B-LF, I-LF...   \n",
       "2  [B-O, B-AC, B-O, B-O, B-O, B-O, B-O, B-O, B-O,...   \n",
       "3                 [B-AC, B-O, B-LF, I-LF, I-LF, B-O]   \n",
       "4  [B-O, B-O, B-AC, B-O, B-O, B-AC, B-O, B-O, B-O...   \n",
       "\n",
       "                                                 ids  \\\n",
       "0      [0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 1, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 1, 0, 0, 0, 0, ...   \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, ...   \n",
       "3                                 [1, 0, 3, 4, 4, 0]   \n",
       "4  [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           sentences  \n",
       "0  For this purpose the Gothenburg Young Persons ...  \n",
       "1  The following physiological traits were measur...  \n",
       "2  Minor H antigen alloimmune responses readily o...  \n",
       "3                        EPI = Echo planar imaging .  \n",
       "4  Furthermore , eNOS - derived NO S - nitrosylat...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': <torchtext.data.field.Field object at 0x177743d90>, 'text': <torchtext.data.field.Field object at 0x177743df0>}\n",
      "['For', 'this', 'purpose', 'the', 'Gothenburg', 'Young', 'Persons', 'Empowerment', 'Scale', '(', 'GYPES', ')', 'was', 'developed', '.']\n",
      "['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'I-LF', 'I-LF', 'I-LF', 'B-O', 'B-AC', 'B-O', 'B-O', 'B-O', 'B-O']\n",
      "Train: 1072\n",
      "Dev: 126\n",
      "Test: 153\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, Dataset, Example\n",
    "\n",
    "text_field = Field(sequential=True, tokenize=lambda x:x, include_lengths=True) # Default behaviour is to tokenize by splitting\n",
    "label_field = Field(sequential=True, tokenize=lambda x:x, is_target=True)\n",
    "\n",
    "fields = {\n",
    "    'sentences': ('text', text_field),\n",
    "    'ids': ('label', label_field)\n",
    "}\n",
    "\n",
    "def read_data(df):\n",
    "    examples = []\n",
    "    fields = {'sentence_labels': ('labels', label_field),\n",
    "              'sentence_tokens': ('text', text_field)}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        tokens = df['tokens'][i]\n",
    "        labels = df['labels'][i]\n",
    "        \n",
    "        e = Example.fromdict({\"sentence_labels\": labels, \"sentence_tokens\": tokens},\n",
    "                             fields=fields)\n",
    "        examples.append(e)\n",
    "    \n",
    "    return Dataset(examples, fields=[('labels', label_field), ('text', text_field)])\n",
    "\n",
    "\n",
    "train_data = read_data(train_dataset)\n",
    "val_data = read_data(val_dataset)\n",
    "test_data = read_data(test_dataset)\n",
    "\n",
    "print(train_data.fields)\n",
    "print(train_data[0].text)\n",
    "print(train_data[0].labels)\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Dev:\", len(val_data))\n",
    "print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "text_field.build_vocab(train_data, max_size=VOCAB_SIZE)\n",
    "label_field.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_iter = BucketIterator(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                            sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "val_iter = BucketIterator(dataset=val_data, batch_size=BATCH_SIZE, \n",
    "                          sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "test_iter = BucketIterator(dataset=test_data, batch_size=BATCH_SIZE, \n",
    "                           sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained embeddings\n",
      "Initializing embedding matrix\n",
      "torch.Size([9135, 300])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "emb = 'fasttext'\n",
    "\n",
    "if emb == 'fasttext':\n",
    "\n",
    "    EMBEDDING_PATH = \"/Users/antoineedy/Documents/MScAI/Semester2/NLP/Coursework/code/data/cc.en.300.vec\"\n",
    "\n",
    "    def load_embeddings(path):\n",
    "        \"\"\" Load the FastText embeddings from the embedding file. \"\"\"\n",
    "        print(\"Loading pre-trained embeddings\")\n",
    "        \n",
    "        embeddings = {}\n",
    "        with open(path) as i:\n",
    "            for line in i:\n",
    "                if len(line) > 2: \n",
    "                    line = line.strip().split()\n",
    "                    word = line[0]\n",
    "                    embedding = np.array(line[1:])\n",
    "                    embeddings[word] = embedding\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "\n",
    "    def initialize_embeddings(embeddings, vocabulary):\n",
    "        \"\"\" Use the pre-trained embeddings to initialize an embedding matrix. \"\"\"\n",
    "        print(\"Initializing embedding matrix\")\n",
    "        embedding_size = len(embeddings[\".\"])\n",
    "        embedding_matrix = np.zeros((len(vocabulary), embedding_size), dtype=np.float32)\n",
    "                                    \n",
    "        for idx, word in enumerate(vocabulary.itos): \n",
    "            if word in embeddings:\n",
    "                embedding_matrix[idx,:] = embeddings[word]\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embeddings = load_embeddings(EMBEDDING_PATH)\n",
    "    embedding_matrix = initialize_embeddings(embeddings, text_field.vocab)\n",
    "    embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "elif emb == 'glove':\n",
    "\n",
    "    EMBEDDING_PATH = \"data/glove.6B.300d.txt\"\n",
    "\n",
    "    def load_embeddings(path):\n",
    "        \"\"\" Load the FastText embeddings from the embedding file. \"\"\"\n",
    "        print(\"Loading pre-trained embeddings\")\n",
    "        \n",
    "        embeddings = {}\n",
    "        with open(path) as i:\n",
    "            for line in i:\n",
    "                if len(line) > 2: \n",
    "                    line = line.strip().split()\n",
    "                    word = line[0]\n",
    "                    embedding = np.array(line[1:])\n",
    "                    embeddings[word] = embedding\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "\n",
    "    def initialize_embeddings(embeddings, vocabulary):\n",
    "        \"\"\" Use the pre-trained embeddings to initialize an embedding matrix. \"\"\"\n",
    "        print(\"Initializing embedding matrix\")\n",
    "        embedding_size = len(embeddings[\".\"])\n",
    "        embedding_matrix = np.zeros((len(vocabulary), embedding_size), dtype=np.float32)\n",
    "                                    \n",
    "        for idx, word in enumerate(vocabulary.itos): \n",
    "            if word in embeddings:\n",
    "                embedding_matrix[idx,:] = embeddings[word]\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embeddings = load_embeddings(EMBEDDING_PATH)\n",
    "    embedding_matrix = initialize_embeddings(embeddings, text_field.vocab)\n",
    "    embedding_matrix = torch.from_numpy(embedding_matrix)\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "\n",
    "elif emb == 'word2vec':\n",
    "    import gensim\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    em = []\n",
    "    for word in text_field.vocab.itos:\n",
    "        if word in model:\n",
    "            em.append(model.get_vector(word))\n",
    "        else:\n",
    "            em.append(np.zeros(300))\n",
    "    em = np.array(em)\n",
    "    embedding_matrix = torch.tensor(em, dtype=torch.float32)\n",
    "    print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size, embeddings=None):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        if embeddings is None:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=1)\n",
    "        \n",
    "        # 3. Optional dropout layer\n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "\n",
    "        # 4. Dense Layer\n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, batch_text, batch_lengths):\n",
    "\n",
    "        embeddings = self.embeddings(batch_text)\n",
    "        \n",
    "        packed_seqs = pack_padded_sequence(embeddings, batch_lengths)\n",
    "        lstm_output, _ = self.lstm(packed_seqs)\n",
    "        lstm_output, _ = pad_packed_sequence(lstm_output)\n",
    "        lstm_output = self.dropout_layer(lstm_output)\n",
    "        \n",
    "        logits = self.hidden2tag(lstm_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6: ['<unk>', '<pad>', 'B-O', 'I-LF', 'B-AC', 'B-LF']\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "def remove_predictions_for_masked_items(predicted_labels, correct_labels): \n",
    "\n",
    "    predicted_labels_without_mask = []\n",
    "    correct_labels_without_mask = []\n",
    "        \n",
    "    for p, c in zip(predicted_labels, correct_labels):\n",
    "        if c > 1:\n",
    "            predicted_labels_without_mask.append(p)\n",
    "            correct_labels_without_mask.append(c)\n",
    "            \n",
    "    return predicted_labels_without_mask, correct_labels_without_mask\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NUM_CLASSES = len(label_field.vocab)\n",
    "print(f\"Number of classes: {NUM_CLASSES}: {label_field.vocab.itos}\")\n",
    "\n",
    "def train(model, train_iter, dev_iter, batch_size, max_epochs, num_batches, patience, output_path):\n",
    "    writer = SummaryWriter()\n",
    "    # add weight to indexes 3, 4, 5\n",
    "    w = [0, 0, 0.0443, 0.6259, 1.0000, 0.4525]\n",
    "    class_weights = torch.tensor(w).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight = class_weights, ignore_index=1)  # we mask the <pad> labels\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_f_score_history = []\n",
    "    dev_f_score_history = []\n",
    "    no_improvement = 0\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            text_length, cur_batch_size = batch.text[0].shape\n",
    "            \n",
    "            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n",
    "            gold = batch.labels.to(device).view(cur_batch_size*text_length)\n",
    "            \n",
    "            loss = criterion(pred, gold)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            \n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "            \n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "            \n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "\n",
    "        train_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n",
    "        train_f_score_history.append(train_scores[2])\n",
    "            \n",
    "        print(\"Total training loss:\", total_loss)\n",
    "        print(\"Training performance:\", train_scores)\n",
    "\n",
    "        #tensorboard\n",
    "        writer.add_scalar('train/loss', total_loss, epoch)\n",
    "        writer.add_scalar('train/precision', train_scores[2], epoch)\n",
    "\n",
    "        \n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        for batch in dev_iter:\n",
    "\n",
    "            text_length, cur_batch_size = batch.text[0].shape\n",
    "\n",
    "            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n",
    "            gold = batch.labels.to(device).view(cur_batch_size * text_length)\n",
    "            loss = criterion(pred, gold)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "            \n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "            \n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "\n",
    "        dev_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n",
    "            \n",
    "        print(\"Total development loss:\", total_loss)\n",
    "        print(\"Development performance:\", dev_scores)\n",
    "\n",
    "        writer.add_scalar('val/loss', total_loss, epoch)\n",
    "        writer.add_scalar('val/precision', dev_scores[2], epoch)\n",
    "\n",
    "        labels = label_field.vocab.itos[2:]\n",
    "        labels = sorted(labels, key=lambda x: x.split(\"-\")[-1])\n",
    "        label_idxs = [label_field.vocab.stoi[l] for l in labels]\n",
    "\n",
    "        cr = classification_report(correct, predictions, labels = label_idxs, target_names=labels, output_dict=True)\n",
    "\n",
    "        out = {}\n",
    "        for key in cr.keys():\n",
    "            if key == 'accuracy':\n",
    "                out[key] = cr[key]\n",
    "            else:\n",
    "                for new_k in ['precision', 'recall', 'f1-score']:\n",
    "                    out[key+'_'+new_k] = cr[key][new_k]\n",
    "        \n",
    "        for (key, value) in out.items():\n",
    "            writer.add_scalar(f'test/{key}', value, epoch)\n",
    "        \n",
    "        dev_f = dev_scores[2]\n",
    "\n",
    "        dev_f = out['macro avg_f1-score']\n",
    "\n",
    "        if len(dev_f_score_history) > patience and dev_f < max(dev_f_score_history):\n",
    "            no_improvement += 1\n",
    "\n",
    "        elif len(dev_f_score_history) == 0 or dev_f > max(dev_f_score_history):\n",
    "            print(\"Saving model.\")\n",
    "            torch.save(model, output_path)\n",
    "            no_improvement = 0\n",
    "            \n",
    "        if no_improvement > patience:\n",
    "            print(\"Macro average F1-score does not improve anymore. Stop training.\")\n",
    "            dev_f_score_history.append(dev_f)\n",
    "            break\n",
    "            \n",
    "        dev_f_score_history.append(dev_f)\n",
    "        \n",
    "    return train_f_score_history, dev_f_score_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_iter, batch_size, labels, target_names): \n",
    "    \n",
    "    total_loss = 0\n",
    "    predictions, correct = [], []\n",
    "    for batch in test_iter:\n",
    "\n",
    "        text_length, cur_batch_size = batch.text[0].shape\n",
    "\n",
    "        pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n",
    "        gold = batch.labels.to(device).view(cur_batch_size * text_length)\n",
    "\n",
    "        _, pred_indices = torch.max(pred, 1)\n",
    "        predicted_labels = list(pred_indices.cpu().numpy())\n",
    "        correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "\n",
    "        predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, correct_labels)\n",
    "\n",
    "        predictions += predicted_labels\n",
    "        correct += correct_labels\n",
    "    \n",
    "    print(classification_report(correct, predictions, labels=labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6 : ['<unk>', '<pad>', 'B-O', 'I-LF', 'B-AC', 'B-LF']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 34/34 [00:04<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 35.392414689064026\n",
      "Training performance: (0.40295, 0.40295, 0.40295, None)\n",
      "Total development loss: 2.5799389481544495\n",
      "Development performance: (0.6926, 0.6926, 0.6926, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 34/34 [00:04<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 18.72599047422409\n",
      "Training performance: (0.7219, 0.7219, 0.7219, None)\n",
      "Total development loss: 2.2425415217876434\n",
      "Development performance: (0.699, 0.699, 0.699, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 34/34 [00:04<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 14.73447397351265\n",
      "Training performance: (0.759775, 0.759775, 0.759775, None)\n",
      "Total development loss: 2.050752341747284\n",
      "Development performance: (0.7342, 0.7342, 0.7342, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 34/34 [00:04<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 12.758317813277245\n",
      "Training performance: (0.774675, 0.774675, 0.774675, None)\n",
      "Total development loss: 1.9445213079452515\n",
      "Development performance: (0.7298, 0.7298, 0.7298, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 34/34 [00:04<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 11.63116604089737\n",
      "Training performance: (0.793225, 0.793225, 0.793225, None)\n",
      "Total development loss: 1.9847281873226166\n",
      "Development performance: (0.6932, 0.6932, 0.6932, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 34/34 [00:05<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 10.406424194574356\n",
      "Training performance: (0.805725, 0.805725, 0.805725, None)\n",
      "Total development loss: 1.8802679479122162\n",
      "Development performance: (0.7698, 0.7698, 0.7698, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 34/34 [00:04<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 9.619245991110802\n",
      "Training performance: (0.813275, 0.813275, 0.813275, None)\n",
      "Total development loss: 1.7861899137496948\n",
      "Development performance: (0.772, 0.772, 0.772, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 34/34 [00:04<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 8.554245814681053\n",
      "Training performance: (0.830925, 0.830925, 0.830925, None)\n",
      "Total development loss: 1.9541434347629547\n",
      "Development performance: (0.7142, 0.7142, 0.7142, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 34/34 [00:04<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 7.967615723609924\n",
      "Training performance: (0.836425, 0.836425, 0.836425, None)\n",
      "Total development loss: 1.8420858681201935\n",
      "Development performance: (0.7956, 0.7956, 0.7956, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 34/34 [00:04<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 7.124230057001114\n",
      "Training performance: (0.8521, 0.8521, 0.8521, None)\n",
      "Total development loss: 1.8376918733119965\n",
      "Development performance: (0.7992, 0.7992, 0.7992, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 34/34 [00:04<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 6.3539725467562675\n",
      "Training performance: (0.861375, 0.861375, 0.861375, None)\n",
      "Total development loss: 1.9091020226478577\n",
      "Development performance: (0.8006, 0.8006, 0.8006, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 34/34 [00:04<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 5.758236363530159\n",
      "Training performance: (0.87095, 0.87095, 0.87095, None)\n",
      "Total development loss: 1.908553570508957\n",
      "Development performance: (0.7912, 0.7912, 0.7912, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 34/34 [00:04<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 5.4631263837218285\n",
      "Training performance: (0.874125, 0.874125, 0.874125, None)\n",
      "Total development loss: 1.9793870449066162\n",
      "Development performance: (0.823, 0.823, 0.823, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 34/34 [00:04<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.573168575763702\n",
      "Training performance: (0.89055, 0.89055, 0.89055, None)\n",
      "Total development loss: 1.9577555358409882\n",
      "Development performance: (0.8202, 0.8202, 0.8202, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 34/34 [00:04<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.409763015806675\n",
      "Training performance: (0.8959, 0.8959, 0.8959, None)\n",
      "Total development loss: 2.2598263025283813\n",
      "Development performance: (0.7864, 0.7864, 0.7864, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 34/34 [00:04<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.393776297569275\n",
      "Training performance: (0.893125, 0.893125, 0.893125, None)\n",
      "Total development loss: 2.1652751863002777\n",
      "Development performance: (0.8096, 0.8096, 0.8096, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 34/34 [00:04<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.054666507989168\n",
      "Training performance: (0.89805, 0.89805, 0.89805, None)\n",
      "Total development loss: 2.1706956326961517\n",
      "Development performance: (0.8248, 0.8248, 0.8248, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 34/34 [00:04<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 3.3185991048812866\n",
      "Training performance: (0.917525, 0.917525, 0.917525, None)\n",
      "Total development loss: 2.268394559621811\n",
      "Development performance: (0.864, 0.864, 0.864, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 34/34 [00:04<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 3.704475622624159\n",
      "Training performance: (0.901975, 0.901975, 0.901975, None)\n",
      "Total development loss: 2.2007787227630615\n",
      "Development performance: (0.8394, 0.8394, 0.8394, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 34/34 [00:05<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 3.397845920175314\n",
      "Training performance: (0.915275, 0.915275, 0.915275, None)\n",
      "Total development loss: 2.349924385547638\n",
      "Development performance: (0.8462, 0.8462, 0.8462, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 34/34 [00:05<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.6212800927460194\n",
      "Training performance: (0.930575, 0.930575, 0.930575, None)\n",
      "Total development loss: 2.9760660529136658\n",
      "Development performance: (0.8834, 0.8834, 0.8834, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 34/34 [00:04<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.6825224719941616\n",
      "Training performance: (0.93185, 0.93185, 0.93185, None)\n",
      "Total development loss: 2.6957974433898926\n",
      "Development performance: (0.8502, 0.8502, 0.8502, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 34/34 [00:05<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 2.032175788655877\n",
      "Training performance: (0.94705, 0.94705, 0.94705, None)\n",
      "Total development loss: 2.7465239763259888\n",
      "Development performance: (0.8598, 0.8598, 0.8598, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 34/34 [00:05<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.741634663194418\n",
      "Training performance: (0.95205, 0.95205, 0.95205, None)\n",
      "Total development loss: 3.3552150428295135\n",
      "Development performance: (0.8872, 0.8872, 0.8872, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 34/34 [00:05<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.6095133610069752\n",
      "Training performance: (0.9588, 0.9588, 0.9588, None)\n",
      "Total development loss: 3.1345724165439606\n",
      "Development performance: (0.8726, 0.8726, 0.8726, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 34/34 [00:04<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.3813468227162957\n",
      "Training performance: (0.9635, 0.9635, 0.9635, None)\n",
      "Total development loss: 3.3928142786026\n",
      "Development performance: (0.8728, 0.8728, 0.8728, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 34/34 [00:04<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.1889613680541515\n",
      "Training performance: (0.967975, 0.967975, 0.967975, None)\n",
      "Total development loss: 3.623875916004181\n",
      "Development performance: (0.8756, 0.8756, 0.8756, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 34/34 [00:05<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.1934564597904682\n",
      "Training performance: (0.966775, 0.966775, 0.966775, None)\n",
      "Total development loss: 3.6892908215522766\n",
      "Development performance: (0.8742, 0.8742, 0.8742, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 34/34 [00:05<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.2792662400752306\n",
      "Training performance: (0.96645, 0.96645, 0.96645, None)\n",
      "Total development loss: 3.6516225934028625\n",
      "Development performance: (0.8782, 0.8782, 0.8782, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 34/34 [00:05<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.0301215220242739\n",
      "Training performance: (0.97455, 0.97455, 0.97455, None)\n",
      "Total development loss: 3.450839400291443\n",
      "Development performance: (0.8592, 0.8592, 0.8592, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 34/34 [00:04<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.9260120308026671\n",
      "Training performance: (0.9756, 0.9756, 0.9756, None)\n",
      "Total development loss: 3.8560178875923157\n",
      "Development performance: (0.8796, 0.8796, 0.8796, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 34/34 [00:05<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.9973677573725581\n",
      "Training performance: (0.97205, 0.97205, 0.97205, None)\n",
      "Total development loss: 3.6911502182483673\n",
      "Development performance: (0.873, 0.873, 0.873, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 34/34 [00:05<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.0568552855402231\n",
      "Training performance: (0.972375, 0.972375, 0.972375, None)\n",
      "Total development loss: 3.735735446214676\n",
      "Development performance: (0.8802, 0.8802, 0.8802, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 34/34 [00:04<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.7159683024510741\n",
      "Training performance: (0.9803, 0.9803, 0.9803, None)\n",
      "Total development loss: 4.46711939573288\n",
      "Development performance: (0.893, 0.893, 0.893, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 34/34 [00:05<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5342136397957802\n",
      "Training performance: (0.986175, 0.986175, 0.986175, None)\n",
      "Total development loss: 4.692116737365723\n",
      "Development performance: (0.8864, 0.8864, 0.8864, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 34/34 [00:04<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.4244922960642725\n",
      "Training performance: (0.989475, 0.989475, 0.989475, None)\n",
      "Total development loss: 4.65263557434082\n",
      "Development performance: (0.8884, 0.8884, 0.8884, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 34/34 [00:04<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3814628953114152\n",
      "Training performance: (0.98995, 0.98995, 0.98995, None)\n",
      "Total development loss: 4.8156720995903015\n",
      "Development performance: (0.8796, 0.8796, 0.8796, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 34/34 [00:04<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3577737919986248\n",
      "Training performance: (0.99115, 0.99115, 0.99115, None)\n",
      "Total development loss: 5.199984312057495\n",
      "Development performance: (0.8848, 0.8848, 0.8848, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 34/34 [00:04<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.542128452565521\n",
      "Training performance: (0.984775, 0.984775, 0.984775, None)\n",
      "Total development loss: 4.5565802454948425\n",
      "Development performance: (0.8808, 0.8808, 0.8808, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 34/34 [00:04<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5277434438467026\n",
      "Training performance: (0.986725, 0.986725, 0.986725, None)\n",
      "Total development loss: 4.8064024448394775\n",
      "Development performance: (0.8832, 0.8832, 0.8832, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 34/34 [00:05<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5420421320013702\n",
      "Training performance: (0.985625, 0.985625, 0.985625, None)\n",
      "Total development loss: 4.905134379863739\n",
      "Development performance: (0.8836, 0.8836, 0.8836, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 34/34 [00:04<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5680480957962573\n",
      "Training performance: (0.986825, 0.986825, 0.986825, None)\n",
      "Total development loss: 4.556818664073944\n",
      "Development performance: (0.8762, 0.8762, 0.8762, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 34/34 [00:05<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.37934650178067386\n",
      "Training performance: (0.991325, 0.991325, 0.991325, None)\n",
      "Total development loss: 4.300981879234314\n",
      "Development performance: (0.8766, 0.8766, 0.8766, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 34/34 [00:04<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.3238329233136028\n",
      "Training performance: (0.99185, 0.99185, 0.99185, None)\n",
      "Total development loss: 5.141140520572662\n",
      "Development performance: (0.8796, 0.8796, 0.8796, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 34/34 [00:05<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.2620882159098983\n",
      "Training performance: (0.99285, 0.99285, 0.99285, None)\n",
      "Total development loss: 6.248545527458191\n",
      "Development performance: (0.8966, 0.8966, 0.8966, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 34/34 [00:04<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.24884821427986026\n",
      "Training performance: (0.99425, 0.99425, 0.99425, None)\n",
      "Total development loss: 6.236972212791443\n",
      "Development performance: (0.897, 0.897, 0.897, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 34/34 [00:04<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.2625348484143615\n",
      "Training performance: (0.993125, 0.993125, 0.993125, None)\n",
      "Total development loss: 5.587915360927582\n",
      "Development performance: (0.8852, 0.8852, 0.8852, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 34/34 [00:04<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.26183820189908147\n",
      "Training performance: (0.9926, 0.9926, 0.9926, None)\n",
      "Total development loss: 6.059037685394287\n",
      "Development performance: (0.8968, 0.8968, 0.8968, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 34/34 [00:05<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.2130005364306271\n",
      "Training performance: (0.99535, 0.99535, 0.99535, None)\n",
      "Total development loss: 5.255974113941193\n",
      "Development performance: (0.884, 0.884, 0.884, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 34/34 [00:05<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.4476106498623267\n",
      "Training performance: (0.98795, 0.98795, 0.98795, None)\n",
      "Total development loss: 4.351088106632233\n",
      "Development performance: (0.8612, 0.8612, 0.8612, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 34/34 [00:05<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.5565261275041848\n",
      "Training performance: (0.984575, 0.984575, 0.984575, None)\n",
      "Total development loss: 4.736869752407074\n",
      "Development performance: (0.8738, 0.8738, 0.8738, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 34/34 [00:04<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.31372399698011577\n",
      "Training performance: (0.9917, 0.9917, 0.9917, None)\n",
      "Total development loss: 5.523014843463898\n",
      "Development performance: (0.8904, 0.8904, 0.8904, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 34/34 [00:05<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.24281271372456104\n",
      "Training performance: (0.994725, 0.994725, 0.994725, None)\n",
      "Total development loss: 5.907220125198364\n",
      "Development performance: (0.8932, 0.8932, 0.8932, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 34/34 [00:04<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.16010872123297304\n",
      "Training performance: (0.996875, 0.996875, 0.996875, None)\n",
      "Total development loss: 5.830480635166168\n",
      "Development performance: (0.8894, 0.8894, 0.8894, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 34/34 [00:05<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.12246986862737685\n",
      "Training performance: (0.9978, 0.9978, 0.9978, None)\n",
      "Total development loss: 6.14190286397934\n",
      "Development performance: (0.898, 0.898, 0.898, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 34/34 [00:04<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.09222115488955751\n",
      "Training performance: (0.9985, 0.9985, 0.9985, None)\n",
      "Total development loss: 6.256701946258545\n",
      "Development performance: (0.8916, 0.8916, 0.8916, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 34/34 [00:04<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.08205203362740576\n",
      "Training performance: (0.998525, 0.998525, 0.998525, None)\n",
      "Total development loss: 6.640747666358948\n",
      "Development performance: (0.896, 0.896, 0.896, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 34/34 [00:04<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.06751919246744365\n",
      "Training performance: (0.998825, 0.998825, 0.998825, None)\n",
      "Total development loss: 6.488456070423126\n",
      "Development performance: (0.8942, 0.8942, 0.8942, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 34/34 [00:05<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.06177984509849921\n",
      "Training performance: (0.999125, 0.999125, 0.999125, None)\n",
      "Total development loss: 6.763927340507507\n",
      "Development performance: (0.8954, 0.8954, 0.8954, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 34/34 [00:05<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.0536094672861509\n",
      "Training performance: (0.999275, 0.999275, 0.999275, None)\n",
      "Total development loss: 6.847843289375305\n",
      "Development performance: (0.8956, 0.8956, 0.8956, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 34/34 [00:04<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.04801679588854313\n",
      "Training performance: (0.9994, 0.9994, 0.9994, None)\n",
      "Total development loss: 7.473007678985596\n",
      "Development performance: (0.896, 0.896, 0.896, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 34/34 [00:04<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.041631550062447786\n",
      "Training performance: (0.999575, 0.999575, 0.999575, None)\n",
      "Total development loss: 7.2200517654418945\n",
      "Development performance: (0.8954, 0.8954, 0.8954, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 34/34 [00:04<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.04230538348201662\n",
      "Training performance: (0.999325, 0.999325, 0.999325, None)\n",
      "Total development loss: 7.543216228485107\n",
      "Development performance: (0.8956, 0.8956, 0.8956, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 34/34 [00:04<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.03852197853848338\n",
      "Training performance: (0.9996, 0.9996, 0.9996, None)\n",
      "Total development loss: 7.6279571652412415\n",
      "Development performance: (0.8988, 0.8988, 0.8988, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 34/34 [00:05<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.037672465085051954\n",
      "Training performance: (0.99945, 0.99945, 0.99945, None)\n",
      "Total development loss: 7.471920311450958\n",
      "Development performance: (0.8978, 0.8978, 0.8978, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 34/34 [00:04<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.043368518294300884\n",
      "Training performance: (0.999275, 0.999275, 0.999275, None)\n",
      "Total development loss: 7.589651584625244\n",
      "Development performance: (0.8946, 0.8946, 0.8946, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 34/34 [00:04<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.041077723901253194\n",
      "Training performance: (0.99945, 0.99945, 0.99945, None)\n",
      "Total development loss: 7.572110056877136\n",
      "Development performance: (0.8952, 0.8952, 0.8952, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 34/34 [00:04<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.03442049180739559\n",
      "Training performance: (0.999575, 0.999575, 0.999575, None)\n",
      "Total development loss: 7.7598336935043335\n",
      "Development performance: (0.8966, 0.8966, 0.8966, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 34/34 [00:04<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.028972914471523836\n",
      "Training performance: (0.99965, 0.99965, 0.99965, None)\n",
      "Total development loss: 7.8118391036987305\n",
      "Development performance: (0.9002, 0.9002, 0.9002, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 34/34 [00:04<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 0.027072568336734548\n",
      "Training performance: (0.999675, 0.999675, 0.999675, None)\n",
      "Total development loss: 8.142091989517212\n",
      "Development performance: (0.8972, 0.8972, 0.8972, None)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "EMBEDDING_DIM = 300 #fasttext & word2vec & glove\n",
    "# EMBEDDING_DIM = 768 #bert\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = len(label_field.vocab)\n",
    "print(f\"Number of classes: {NUM_CLASSES} : {label_field.vocab.itos}\")\n",
    "MAX_EPOCHS = 70\n",
    "PATIENCE = 50\n",
    "OUTPUT_PATH = \"model_saves/bilstmtagger\"\n",
    "num_batches = math.ceil(len(train_data) / BATCH_SIZE)\n",
    "\n",
    "tagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES, embeddings=embedding_matrix)  # embeddings\n",
    "# tagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES)  # no embeddings\n",
    "\n",
    "train_f, dev_f = train(tagger.to(device), train_iter, val_iter, BATCH_SIZE, MAX_EPOCHS, \n",
    "                       num_batches, PATIENCE, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "df = pd.DataFrame({'epochs': range(0,len(train_f)), \n",
    "                  'train_f': train_f, \n",
    "                   'val_f': dev_f})\n",
    " \n",
    "# multiple line plot\n",
    "plt.plot('epochs', 'train_f', data=df, color='blue', linewidth=2)\n",
    "plt.plot('epochs', 'val_f', data=df, color='green', linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"model_saves/bilstmtagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger(\n",
       "  (embeddings): Embedding(9135, 300)\n",
       "  (lstm): LSTM(300, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = torch.load(OUTPUT_PATH)\n",
    "tagger.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        B-AC       0.45      0.71      0.55       270\n",
      "        I-LF       0.60      0.73      0.66       288\n",
      "        B-LF       0.48      0.57      0.52       150\n",
      "         B-O       0.96      0.90      0.93      4292\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.62      0.73      0.67      5000\n",
      "weighted avg       0.90      0.87      0.88      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = label_field.vocab.itos[2:]\n",
    "labels = sorted(labels, key=lambda x: x.split(\"-\")[-1])\n",
    "label_idxs = [label_field.vocab.stoi[l] for l in labels]\n",
    "\n",
    "test(tagger, test_iter, BATCH_SIZE, labels = label_idxs, target_names = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Back, Style\n",
    "\n",
    "def vizu(words, output, truth):\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        output = output.squeeze().tolist()\n",
    "    col = {0: Back.GREEN, 1: Back.RED, 2: Back.BLACK, 3: Back.BLUE, 4: Back.MAGENTA}\n",
    "    colors1 = [col[i] for i in output]\n",
    "    colors2 = [col[i] for i in truth]\n",
    "    words = [word.replace(\"Ġ\", \"\") for word in words]\n",
    "    print(Style.RESET_ALL + \"Output:\")\n",
    "    for i, word in enumerate(words):\n",
    "        print(colors1[i] + word, end=\" \")\n",
    "    print(Style.RESET_ALL + \"\\nTruth:\")\n",
    "    for i, word in enumerate(words):\n",
    "        print(colors2[i] + word, end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
